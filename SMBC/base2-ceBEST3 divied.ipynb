{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, TargetEncoder\n",
    "import lightgbm as lgb\n",
    "# import optuna.integration.lightgbm as lgb\n",
    "\n",
    "import category_encoders as ce\n",
    "import math\n",
    "\n",
    "# pandasの行を省略しない\n",
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/python/signate/data/train.csv', index_col=0)\n",
    "test_df = pd.read_csv('C:/python/signate/data/test.csv', index_col=0)\n",
    "sample_df = pd.read_csv('C:/python/signate/data/sample_submission.csv', header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データはほとんどカテゴリカル\n",
    "#### 思いついた精度向上案\n",
    "- commonを個別名称として、latinの上を属名として利用する\n",
    "- 地区が多すぎるので整理する\n",
    "- 季節性を導入\n",
    "- 郵便番号は連続性があるからカテゴリカルにしないほうがよいのでは\n",
    "- 高い確率で状態が悪いと推測された木の近くの木はフラグ立てる。つまり2段階モデル\n",
    "- 同じ人が記録した場合、1日のうちの0,1,2の割合は無意識にバイアスがかかって同じくらいにしてしまうのでは？\n",
    "- 曜日の導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(df):\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    # df['cos_day'] = df['created_at'].dt.dayofyear\n",
    "    # df['cos_day'] = df['cos_day'].apply(lambda x: np.cos(math.radians(90 - (x/365)*365)))\n",
    "    # df['sin_day'] = df['created_at'].dt.dayofyear\n",
    "    # df['sin_day'] = df['sin_day'].apply(lambda x: np.sin(math.radians(90 - (x/365)*365)))  \n",
    "    # df['year'] = df['created_at'].dt.year\n",
    "    df['month'] = df['created_at'].dt.month\n",
    "    # df['weekday'] = df['created_at'].dt.weekday\n",
    "    # df['day'] = df.created_at.dt.day\n",
    "    # df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    # df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    # df.drop('month', axis=1, inplace=True)\n",
    "    df.drop('created_at', axis=1, inplace=True)\n",
    "    df['steward'].fillna('0', inplace=True)\n",
    "    df['guards'].fillna('0', inplace=True)\n",
    "    df['problems'].fillna('NoProblem', inplace=True)\n",
    "    df['spc_genus'] = df['spc_latin'].str.split(' ').str[0]\n",
    "    df.drop('spc_latin', axis=1, inplace=True)\n",
    "\n",
    "    # df['curb_loc'] = df['curb_loc'].map({'OnCurb':3, 'OffsetFromCurb':1})\n",
    "    # df['sidewalk'] = df['sidewalk'].map({'NoDamage':1, 'Damage':3})\n",
    "    # df['guards'] = df['guards'].map({'Helpful':1, 'Harmful':3, '0':2, 'unsure':2})\n",
    "    # df['status_point'] = df['curb_loc'] * df['sidewalk'] * df['guards']\n",
    "    return df\n",
    "\n",
    "train_clean = cleansing(train_df)\n",
    "test_clean = cleansing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koba_\\AppData\\Local\\Temp\\ipykernel_19108\\1467932340.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_clean.drop('health', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 種目ごとの平均直径とその差を特徴量にする\n",
    "test_clean['health'] = 0\n",
    "all_df = pd.concat([train_clean, test_clean], axis=0)   \n",
    "tmp_df = all_df[['tree_dbh', 'spc_common', ]]\n",
    "dbh_mean_common = (pd.DataFrame(tmp_df.groupby('spc_common').mean()['tree_dbh']))\n",
    "dbh_mean_common.rename(columns={'tree_dbh': 'dbh_mean_common'}, inplace=True)\n",
    "all_df = pd.merge(all_df, dbh_mean_common, on='spc_common', how='left')\n",
    "# all_df['dbh_diff_common'] = all_df['tree_dbh'] - all_df['dbh_mean_common']\n",
    "\n",
    "tmp_df = all_df[['tree_dbh', 'cb_num']]\n",
    "dbh_mean_cb = (pd.DataFrame(tmp_df.groupby('cb_num').mean()['tree_dbh']))\n",
    "dbh_mean_cb.rename(columns={'tree_dbh': 'dbh_mean_cb'}, inplace=True)\n",
    "all_df = pd.merge(all_df, dbh_mean_cb, on='cb_num', how='left')\n",
    "\n",
    "# all_df['diff'] = all_df['dbh_mean_common'] - all_df['tree_dbh']\n",
    "\n",
    "train_clean = all_df.iloc[:len(train_clean)]\n",
    "test_clean = all_df.iloc[len(train_clean):]\n",
    "test_clean.drop('health', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# trainとtestに含まれるproblemは全く同じ\\n# problemのonehotカラムを用意\\nproblem_list = ['BranchLights', 'BranchOther', 'MetalGrates', 'RootOther', 'Stones', 'Sneakers', 'TrunkLights',\\n                 'TrunkOther', 'WiresRope', 'NoProblem']\\t\\n\\nfor problem in problem_list:\\n    train_clean.loc[:, problem] = 0\\n    test_clean.loc[:, problem] = 0\\n\\ntrain_clean['problem_count'] = 0\\ntest_clean['problem_count'] = 0\\n\\n# problemlistにあったらonehotする\\nfor i in train_clean.index:\\n    p_count = 0\\n    for problem in problem_list:\\n        if(problem in train_clean.loc[i, 'problems']):\\n            train_clean.loc[i, problem] = 1\\n            if(problem != 'NoProblem'):\\n                p_count+=1\\n    train_clean.loc[i, 'problem_count'] = p_count\\n\\nfor i in test_clean.index:\\n    p_count = 0\\n    for problem in problem_list:\\n        if(problem in test_clean.loc[i, 'problems']):\\n            test_clean.loc[i, problem] = 1\\n            if(problem != 'NoProblem'):\\n                p_count+=1\\n    test_clean.loc[i, 'problem_count'] = p_count\\n\\ntest_clean.head()\\n\""
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# trainとtestに含まれるproblemは全く同じ\n",
    "# problemのonehotカラムを用意\n",
    "problem_list = ['BranchLights', 'BranchOther', 'MetalGrates', 'RootOther', 'Stones', 'Sneakers', 'TrunkLights',\n",
    "                 'TrunkOther', 'WiresRope', 'NoProblem']\t\n",
    "\n",
    "for problem in problem_list:\n",
    "    train_clean.loc[:, problem] = 0\n",
    "    test_clean.loc[:, problem] = 0\n",
    "\n",
    "train_clean['problem_count'] = 0\n",
    "test_clean['problem_count'] = 0\n",
    "\n",
    "# problemlistにあったらonehotする\n",
    "for i in train_clean.index:\n",
    "    p_count = 0\n",
    "    for problem in problem_list:\n",
    "        if(problem in train_clean.loc[i, 'problems']):\n",
    "            train_clean.loc[i, problem] = 1\n",
    "            if(problem != 'NoProblem'):\n",
    "                p_count+=1\n",
    "    train_clean.loc[i, 'problem_count'] = p_count\n",
    "\n",
    "for i in test_clean.index:\n",
    "    p_count = 0\n",
    "    for problem in problem_list:\n",
    "        if(problem in test_clean.loc[i, 'problems']):\n",
    "            test_clean.loc[i, problem] = 1\n",
    "            if(problem != 'NoProblem'):\n",
    "                p_count+=1\n",
    "    test_clean.loc[i, 'problem_count'] = p_count\n",
    "\n",
    "test_clean.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## やりたい実験\n",
    "- あらゆるエンコーディング方法の比較 ok\n",
    "- 地区ごとの予測モデルの実装 \n",
    "- 365日で一周するsin, cos 意味無し\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error! categorical is not same\n",
      "足りないカラム: {'month', 'year'}\n"
     ]
    }
   ],
   "source": [
    "# エンコーディングパート\n",
    "\n",
    "# すべてのカテゴリカル変数\n",
    "all_categorical_cols = ['curb_loc', 'steward', 'guards', 'sidewalk',\n",
    "       'user_type', 'problems', 'spc_common', 'nta', 'nta_name', 'borocode',\n",
    "       'boro_ct', 'boroname', 'zip_city', 'cb_num', 'st_senate', 'st_assem',\n",
    "       'cncldist', 'year', 'month',  'spc_genus']\n",
    "\n",
    "# \n",
    "\n",
    "# 落とすカラム\n",
    "drop_cols = ['nta_name', 'boroname']#'',\n",
    "# カウントエンコーディング\n",
    "ce_columns = ['curb_loc', 'steward', 'guards', 'sidewalk',\n",
    "       'user_type', 'problems', 'spc_common', 'nta', \n",
    "              'borocode', 'boro_ct',  'zip_city', 'cb_num', 'st_senate',\n",
    "       'st_assem', 'cncldist', 'spc_genus', ]\n",
    "# 'nta',\n",
    "#  \n",
    "# \n",
    "\n",
    "# ターゲットエンコーディング\n",
    "te_columns = []\n",
    "# ラベルエンコーディング\n",
    "le_columns = []\n",
    "encoding_cals = list(drop_cols + ce_columns + te_columns + drop_cols)\n",
    "if(encoding_cals != all_categorical_cols):\n",
    "    print('error! categorical is not same')\n",
    "    print('足りないカラム:', set(all_categorical_cols)-set(encoding_cals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットエンコーディングパート\n",
    "if(len(te_columns) > 0):\n",
    "    # onehot正解ラベルの作成\n",
    "    for i in range(3):\n",
    "        train_clean['onehot' + str(i)] = 0\n",
    "        train_clean['onehot' + str(i)] = train_clean['onehot' + str(i)].mask(train_clean['health'] == i, 1)\n",
    "\n",
    "    # ターゲットエンコーディングのカラムを3つずつに分ける\n",
    "    te_columns_list = []\n",
    "\n",
    "    for te_column in te_columns:\n",
    "        tmp_list = []\n",
    "        for i in range(3):\n",
    "            train_clean[te_column + '-te' + str(i)] = train_clean[te_column]\n",
    "            test_clean[te_column + '-te' + str(i)] = test_clean[te_column]\n",
    "            tmp_list.append(te_column + '-te' + str(i))\n",
    "        te_columns_list.append(tmp_list)\n",
    "        train_clean.drop(te_column, axis=1, inplace=True)\n",
    "        test_clean.drop(te_column, axis=1, inplace=True)\n",
    "\n",
    "    te_columns_list = np.array(te_columns_list)\n",
    "\n",
    "    # ターゲットエンコーディング\n",
    "    for i in range(3):\n",
    "        te_target = te_columns_list[:, i]\n",
    "        target_enc = TargetEncoder(target_type='binary', random_state=42)\n",
    "        \n",
    "        train_clean[te_target] = target_enc.fit_transform(train_clean[te_target], train_clean['onehot' + str(i)])\n",
    "        # テストデータにtransform\n",
    "        test_clean[te_target]  = target_enc.transform(test_clean[te_target])\n",
    "\n",
    "    # onehot正解ラベルを削除\n",
    "    for i in range(3):\n",
    "        dropcol = 'onehot' + str(i)\n",
    "        train_clean.drop(dropcol, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# 目的変数を指定\n",
    "y = train_clean['health']\n",
    "\n",
    "# 使わないcolumnを削除\n",
    "train_clean_drop = train_clean.drop(['health'], axis=1)\n",
    "train_clean_drop = train_clean_drop.drop(drop_cols, axis=1)\n",
    "test_clean_drop = test_clean.drop(drop_cols,axis=1)\n",
    "\n",
    "# ラベルエンコーディングパート\n",
    "le = LabelEncoder()\n",
    "for column in le_columns:\n",
    "    train_clean_drop[column] = le.fit_transform(train_clean_drop[column])\n",
    "    test_clean_drop[column] = le.transform(test_clean_drop[column])\n",
    "\n",
    "# カテゴリカル変数を指定\n",
    "for col in le_columns:\n",
    "    train_clean_drop[col] = train_clean_drop[col].astype('category')\n",
    "    test_clean_drop[col] = test_clean_drop[col].astype('category')  \n",
    "\n",
    "# カウントエンコーディングパート\n",
    "# mergeしてfit, そのあとtransform\n",
    "all_df = pd.concat([train_clean_drop, test_clean_drop], axis=0)\n",
    "\n",
    "# count encoding\n",
    "cencoder = ce.CountEncoder(cols=ce_columns)\n",
    "cencoder.fit(all_df)\n",
    "\n",
    "# count encodingを適用\n",
    "train_clean_drop = cencoder.transform(train_clean_drop)\n",
    "test_clean_drop = cencoder.transform(test_clean_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0 start\n",
      "[LightGBM] [Info] Number of positive: 15428, number of negative: 559\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 15987, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[475]\ttraining's binary_logloss: 0.0466128\tvalid_1's binary_logloss: 0.221791\n",
      "fold0 f1_score: 0.5076\n",
      "[LightGBM] [Info] Number of positive: 12600, number of negative: 2828\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 974\n",
      "[LightGBM] [Info] Number of data points in the train set: 15428, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[612]\ttraining's binary_logloss: 0.269524\tvalid_1's binary_logloss: 0.602424\n",
      "fold0 f1_score: 0.5131\n",
      "fold1 start\n",
      "[LightGBM] [Info] Number of positive: 15429, number of negative: 558\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 978\n",
      "[LightGBM] [Info] Number of data points in the train set: 15987, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[501]\ttraining's binary_logloss: 0.0399275\tvalid_1's binary_logloss: 0.216703\n",
      "fold1 f1_score: 0.5031\n",
      "[LightGBM] [Info] Number of positive: 12601, number of negative: 2828\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 973\n",
      "[LightGBM] [Info] Number of data points in the train set: 15429, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[673]\ttraining's binary_logloss: 0.254341\tvalid_1's binary_logloss: 0.59765\n",
      "fold1 f1_score: 0.5324\n",
      "fold2 start\n",
      "[LightGBM] [Info] Number of positive: 15429, number of negative: 558\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 973\n",
      "[LightGBM] [Info] Number of data points in the train set: 15987, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[466]\ttraining's binary_logloss: 0.0474692\tvalid_1's binary_logloss: 0.208254\n",
      "fold2 f1_score: 0.5188\n",
      "[LightGBM] [Info] Number of positive: 12601, number of negative: 2828\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 971\n",
      "[LightGBM] [Info] Number of data points in the train set: 15429, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[599]\ttraining's binary_logloss: 0.278671\tvalid_1's binary_logloss: 0.599056\n",
      "fold2 f1_score: 0.5173\n",
      "fold3 start\n",
      "[LightGBM] [Info] Number of positive: 15429, number of negative: 558\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 15987, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[512]\ttraining's binary_logloss: 0.041283\tvalid_1's binary_logloss: 0.229446\n",
      "fold3 f1_score: 0.4994\n",
      "[LightGBM] [Info] Number of positive: 12601, number of negative: 2828\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 976\n",
      "[LightGBM] [Info] Number of data points in the train set: 15429, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\ttraining's binary_logloss: 0.263953\tvalid_1's binary_logloss: 0.593061\n",
      "fold3 f1_score: 0.5146\n",
      "fold4 start\n",
      "[LightGBM] [Info] Number of positive: 15429, number of negative: 559\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 15988, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[494]\ttraining's binary_logloss: 0.0449816\tvalid_1's binary_logloss: 0.209394\n",
      "fold4 f1_score: 0.5358\n",
      "[LightGBM] [Info] Number of positive: 12601, number of negative: 2828\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 973\n",
      "[LightGBM] [Info] Number of data points in the train set: 15429, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[600]\ttraining's binary_logloss: 0.285611\tvalid_1's binary_logloss: 0.586878\n",
      "fold4 f1_score: 0.5187\n",
      "first-CV: 0.5129\n",
      "second-CV: 0.5192\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([train_clean_drop, y], axis=1)\n",
    "origin_df = pd.read_csv('C:/python/signate/data/train.csv', index_col=0)\n",
    "group_num_big = 1\n",
    "group_num_small = 0\n",
    "solo_num = 2\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "first_valid_scores = []\n",
    "second_valid_scores = []\n",
    "first_models = []\n",
    "second_models = []\n",
    "# train_dfと同じindexを持つデータフレームを作成\n",
    "lgb_pred_proba = origin_df[['health']].copy()\n",
    "lgb_pred_proba.rename(columns= {'health':'lgb'}, inplace=True)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df, y)):\n",
    "    print(f'fold{fold} start')\n",
    "\n",
    "    # 0,2と1の予測モデル\n",
    "    X_train = train_df.iloc[train_idx]\n",
    "    X_valid = train_df.iloc[valid_idx]\n",
    "\n",
    "    # 元々のyを保持\n",
    "    y_all_train = X_train['health']\n",
    "    y_all_valid = X_valid['health']\n",
    "\n",
    "    # 0,2のhelathを0とする\n",
    "    X_train_first = X_train.copy()\n",
    "    X_valid_first = X_valid.copy()\n",
    "\n",
    "    # groupは0,soloは1にする\n",
    "    X_train_first['health'] = X_train_first['health'].mask(X_train_first['health'] == group_num_big, 1)\n",
    "    X_valid_first['health'] = X_valid_first['health'].mask(X_valid_first['health'] == group_num_big, 1)\n",
    "    X_train_first['health'] = X_train_first['health'].mask(X_train_first['health'] == group_num_small, 1)\n",
    "    X_valid_first['health'] = X_valid_first['health'].mask(X_valid_first['health'] == group_num_small, 1)\n",
    "    X_train_first['health'] = X_train_first['health'].mask(X_train_first['health'] == solo_num, 0)\n",
    "    X_valid_first['health'] = X_valid_first['health'].mask(X_valid_first['health'] == solo_num, 0)\n",
    "\n",
    "    # yを分離\n",
    "    y_train_first = X_train_first['health']\n",
    "    X_train_first_drop = X_train_first.drop('health', axis=1)\n",
    "    y_valid_first = X_valid_first['health']\n",
    "    X_valid_first_drop = X_valid_first.drop('health', axis=1)\n",
    "\n",
    "    # 1段目の予測\n",
    "    lgb_train = lgb.Dataset(X_train_first_drop, y_train_first, weight=compute_sample_weight(class_weight='balanced', y=y_train_first).astype('float32'))\n",
    "    lgb_eval = lgb.Dataset(X_valid_first_drop, y_valid_first, weight=np.ones(len(X_valid_first)).astype('float32'))\n",
    "\n",
    "    verbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\n",
    "    params = {'objective': 'binary',\n",
    "    'metric': 'binary_logloss', 'seed': 0,}\n",
    "    \n",
    "    first_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      valid_sets=[lgb_train, lgb_eval],\n",
    "                      num_boost_round=1000,\n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=50, \n",
    "                                verbose=True), # early_stopping用コールバック関数\n",
    "                           lgb.log_evaluation(verbose_eval)], # コマンドライン出力用コールバック関数))\n",
    "                        # feval=f1,\n",
    "                        # categorical_feature=le_columns,\n",
    "                    )\n",
    "\n",
    "    y_pred = first_model.predict(X_valid_first_drop, num_iteration=first_model.best_iteration)\n",
    "    # lgb_pred_proba.iloc[valid_idx] = y_pred\n",
    "    # 予測結果の2値化\n",
    "    y_pred_max = np.where(y_pred < 0.5, 0, 1)\n",
    "    score = f1_score(y_valid_first, y_pred_max, average='macro')\n",
    "    print(f'fold{fold} f1_score: {score:.4f}')\n",
    "    first_valid_scores.append(score)\n",
    "    first_models.append(first_model)\n",
    "\n",
    "    # 0と2の予測\n",
    "    X_train_second = X_train.copy()\n",
    "    X_valid_second = X_valid.copy()\n",
    "    # groupnumを抽出\n",
    "    X_train_second = X_train_second[X_train_second['health'] != solo_num]\n",
    "    X_valid_second = X_valid_second[X_valid_second['health'] != solo_num]\n",
    "\n",
    "    # yをmask\n",
    "    X_train_second['health'] = X_train_second['health'].mask(X_train_second['health'] == group_num_big, 1)\n",
    "    X_train_second['health'] = X_train_second['health'].mask(X_train_second['health'] == group_num_small, 0)\n",
    "    X_valid_second['health'] = X_valid_second['health'].mask(X_valid_second['health'] == group_num_big, 1)\n",
    "    X_valid_second['health'] = X_valid_second['health'].mask(X_valid_second['health'] == group_num_small, 0)\n",
    "\n",
    "    # yを分離\n",
    "    y_train_second = X_train_second['health']\n",
    "    X_train_second_drop = X_train_second.drop('health', axis=1)\n",
    "    y_valid_second = X_valid_second['health']\n",
    "    X_valid_second_drop = X_valid_second.drop('health', axis=1, )\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train_second_drop, y_train_second, weight=compute_sample_weight(class_weight='balanced', y=y_train_second).astype('float32'))\n",
    "    lgb_eval = lgb.Dataset(X_valid_second_drop, y_valid_second, weight=np.ones(len(X_valid_second)).astype('float32'))\n",
    "\n",
    "    verbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\n",
    "    params = {'objective': 'binary',\n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': 0,}\n",
    "    \n",
    "    second_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      valid_sets=[lgb_train, lgb_eval],\n",
    "                      num_boost_round=1000,\n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=50, \n",
    "                                verbose=True), # early_stopping用コールバック関数\n",
    "                           lgb.log_evaluation(verbose_eval)], # コマンドライン出力用コールバック関数))\n",
    "                    )\n",
    "\n",
    "    y_pred = second_model.predict(X_valid_second_drop, num_iteration=second_model.best_iteration)\n",
    "    # lgb_pred_proba.iloc[valid_idx] = y_pred\n",
    "    y_pred_max = np.where(y_pred<0.5, 0, 1)\n",
    "    score = f1_score(y_valid_second, y_pred_max, average='macro')\n",
    "    print(f'fold{fold} f1_score: {score:.4f}')\n",
    "    second_valid_scores.append(score)\n",
    "    second_models.append(second_model)\n",
    "print(f'first-CV: {np.mean(first_valid_scores):.4f}')\n",
    "print(f'second-CV: {np.mean(second_valid_scores):.4f}')\n",
    "# lgb_pred_proba.to_csv('C:/python/signate/data/lgb_pred_proba.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0 f1_score: 0.3482\n",
      "fold1 f1_score: 0.3559\n",
      "fold2 f1_score: 0.3578\n",
      "fold3 f1_score: 0.3431\n",
      "fold4 f1_score: 0.3693\n",
      "CV: 0.3549\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lgb\n",
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルで評価\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "second_valid_scores = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df, y)):\n",
    "    X_train = train_df.iloc[train_idx]\n",
    "    X_valid = train_df.iloc[valid_idx]\n",
    "    y_valid = X_valid['health']\n",
    "    X_valid_first = X_valid.copy()\n",
    "    X_valid_first.drop('health', axis=1, inplace=True)\n",
    "\n",
    "    # 1段目の予測\n",
    "    first_model = first_models[fold]\n",
    "    y_pred_first = first_model.predict(X_valid_first, num_iteration=first_model.best_iteration)\n",
    "    # 予測結果の2値化\n",
    "    y_pred_first = np.where(y_pred_first < 0.5, solo_num, 1)\n",
    "    \n",
    "    # 0, 2の予測\n",
    "    # 1段目の予測結果を結合\n",
    "    X_valid_second = pd.concat([X_valid_first, pd.Series(y_pred_first, index=X_valid_first.index)], axis=1)\n",
    "    # 使うのは1段目で0と予測したデータ    \n",
    "    X_valid_second_target = X_valid_second[X_valid_second[0] == 1]\n",
    "    X_valid_second_drop = X_valid_second_target.drop(0, axis=1)\n",
    "    \n",
    "    # 2段目の予測\n",
    "    second_model = second_models[fold]\n",
    "    y_pred_second = second_model.predict(X_valid_second_drop, num_iteration=second_model.best_iteration)\n",
    "    # 予測結果の2値化は2と0\n",
    "    y_pred_second = np.where(y_pred_second > 0.5, group_num_big, group_num_small)\n",
    "    \n",
    "    # 1段目で0と予測したデータに2段目の予測結果を代入\n",
    "    y_pred = pd.Series(y_pred_first, index=X_valid_first.index)\n",
    "    y_pred[y_pred == 1] = y_pred_second\n",
    "    score = f1_score(y_valid, y_pred, average='macro')\n",
    "    print(f'fold{fold} f1_score: {score:.4f}')\n",
    "    lgb_pred_proba.loc[valid_idx, 'lgb'] = y_pred\n",
    "    second_valid_scores.append(score)\n",
    "    \n",
    "print(f'CV: {np.mean(second_valid_scores):.4f}')\n",
    "lgb_pred_proba.to_csv('C:/python/signate/data/lgb_dive-train.csv')\n",
    "lgb_pred_proba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータデータで予測\n",
    "# 1段目の予測\n",
    "first_model = first_models[4]\n",
    "y_pred_first = first_model.predict(test_clean_drop, num_iteration=first_model.best_iteration)\n",
    "# 予測結果の2値化\n",
    "y_pred_first = np.where(y_pred_first < 0.5, solo_num, 1)\n",
    "\n",
    "# 0, 2の予測\n",
    "# 1段目の予測結果を結合\n",
    "test_second = pd.concat([test_clean_drop, pd.Series(y_pred_first, index=test_clean_drop.index)], axis=1)\n",
    "# 使うのは1段目で0と予測したデータ\n",
    "test_second_target = test_second[test_second[0] == 1]\n",
    "test_second_drop = test_second_target.drop(0, axis=1)\n",
    "# 2段目の予測\n",
    "second_model = second_models[4]\n",
    "y_pred_second = second_model.predict(test_second_drop, num_iteration=second_model.best_iteration)\n",
    "# 予測結果の2値化は2と0\n",
    "y_pred_second_th = np.where(y_pred_second > 0.5, group_num_big, group_num_small)\n",
    "\n",
    "# 1段目で0と予測したデータに2段目の予測結果を代入\n",
    "y_pred_series = pd.Series(y_pred_first, index=test_clean_drop.index)\n",
    "y_pred_series_copy = y_pred_series.copy()\n",
    "y_pred_series_copy[y_pred_series_copy == 1] = y_pred_second_th\n",
    "y_pred_series_copy.index = sample_df.index\n",
    "y_pred_series_copy.to_csv('C:/python/signate/data/lgb-dive_2to0.9.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "19984    0\n",
       "19985    1\n",
       "19986    0\n",
       "19987    1\n",
       "19988    0\n",
       "        ..\n",
       "39964    1\n",
       "39965    1\n",
       "39966    1\n",
       "39967    1\n",
       "39968    2\n",
       "Length: 19702, dtype: int32"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_series_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlgb_train = lgb.Dataset(train_clean_drop, y, weight=compute_sample_weight(class_weight='balanced', y=y).astype('float32'))\\nverbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\\nparams = {'objective': 'multiclass',\\n'metric': 'multi_logloss',\\n'num_class': 3,\\n'seed': 0\\n}\\n\\nmodel = lgb.train(params,\\n                    lgb_train,\\n                    num_boost_round=520,\\n                )\\n\""
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全データを利用して学習\n",
    "'''\n",
    "lgb_train = lgb.Dataset(train_clean_drop, y, weight=compute_sample_weight(class_weight='balanced', y=y).astype('float32'))\n",
    "verbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\n",
    "params = {'objective': 'multiclass',\n",
    "'metric': 'multi_logloss',\n",
    "'num_class': 3,\n",
    "'seed': 0\n",
    "}\n",
    "\n",
    "model = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=520,\n",
    "                )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[327], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m lgb_pred_proba_test \u001b[38;5;241m=\u001b[39m test_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborocode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree_dbh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnta\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      2\u001b[0m lgb_pred_proba_test\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborocode\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb-0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree_dbh\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb-1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnta\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb-2\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(test_clean_drop)\n\u001b[0;32m      5\u001b[0m lgb_pred_proba_test\u001b[38;5;241m.\u001b[39miloc[:] \u001b[38;5;241m=\u001b[39m y_pred[:]\n\u001b[0;32m      6\u001b[0m lgb_pred_proba_test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/python/signate/data/lgb_pred_proba_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "lgb_pred_proba_test = test_df[['borocode', 'tree_dbh', 'nta']].copy()\n",
    "lgb_pred_proba_test.rename(columns= {'borocode':'lgb-0', 'tree_dbh':'lgb-1', 'nta':'lgb-2'}, inplace=True)\n",
    "\n",
    "y_pred = model.predict(test_clean_drop)\n",
    "lgb_pred_proba_test.iloc[:] = y_pred[:]\n",
    "lgb_pred_proba_test.to_csv('C:/python/signate/data/lgb_pred_proba_test.csv')\n",
    "y_pred_max = np.argmax(y_pred, axis=1) # 最尤と判断したクラスの値にする\n",
    "sample_df[1] = y_pred_max\n",
    "# sample_df.to_csv('C:/python/signate/data/base3(cv0.3586).csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tree_dbh</td>\n",
       "      <td>1763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>boro_ct</td>\n",
       "      <td>1692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nta</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spc_common</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>problems</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dbh_mean_common</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spc_genus</td>\n",
       "      <td>853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>month_cos</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>st_assem</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>month_sin</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dbh_mean_cb</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cb_num</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cncldist</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zip_city</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>st_senate</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user_type</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guards</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steward</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sidewalk</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>curb_loc</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>borocode</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature  Importance\n",
       "0          tree_dbh        1763\n",
       "10          boro_ct        1692\n",
       "8               nta        1177\n",
       "7        spc_common        1094\n",
       "6          problems         990\n",
       "19  dbh_mean_common         880\n",
       "18        spc_genus         853\n",
       "17        month_cos         752\n",
       "14         st_assem         723\n",
       "16        month_sin         683\n",
       "20      dbh_mean_cb         651\n",
       "12           cb_num         587\n",
       "15         cncldist         547\n",
       "11         zip_city         516\n",
       "13        st_senate         499\n",
       "5         user_type         440\n",
       "3            guards         322\n",
       "2           steward         300\n",
       "4          sidewalk         186\n",
       "1          curb_loc         155\n",
       "9          borocode         100"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importance = first_model.feature_importance()\n",
    "feature_names = first_model.feature_name()\n",
    "\n",
    "df_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "# importtane順に並び替え\n",
    "df_importance = df_importance.sort_values('Importance', ascending=False)\n",
    "df_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
