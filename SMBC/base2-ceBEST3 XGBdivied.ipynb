{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, TargetEncoder\n",
    "from xgboost import XGBClassifier\n",
    "# import optuna.integration.lightgbm as lgb\n",
    "\n",
    "import category_encoders as ce\n",
    "import math\n",
    "\n",
    "# pandasの行を省略しない\n",
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/python/signate/data/train.csv', index_col=0)\n",
    "test_df = pd.read_csv('C:/python/signate/data/test.csv', index_col=0)\n",
    "sample_df = pd.read_csv('C:/python/signate/data/sample_submission.csv', header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データはほとんどカテゴリカル\n",
    "#### 思いついた精度向上案\n",
    "- commonを個別名称として、latinの上を属名として利用する\n",
    "- 地区が多すぎるので整理する\n",
    "- 季節性を導入\n",
    "- 郵便番号は連続性があるからカテゴリカルにしないほうがよいのでは\n",
    "- 高い確率で状態が悪いと推測された木の近くの木はフラグ立てる。つまり2段階モデル\n",
    "- 同じ人が記録した場合、1日のうちの0,1,2の割合は無意識にバイアスがかかって同じくらいにしてしまうのでは？\n",
    "- 曜日の導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(df):\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    # df['cos_day'] = df['created_at'].dt.dayofyear\n",
    "    # df['cos_day'] = df['cos_day'].apply(lambda x: np.cos(math.radians(90 - (x/365)*365)))\n",
    "    # df['sin_day'] = df['created_at'].dt.dayofyear\n",
    "    # df['sin_day'] = df['sin_day'].apply(lambda x: np.sin(math.radians(90 - (x/365)*365)))  \n",
    "    # df['year'] = df['created_at'].dt.year\n",
    "    df['month'] = df['created_at'].dt.month\n",
    "    # df['weekday'] = df['created_at'].dt.weekday\n",
    "    # df['day'] = df.created_at.dt.day\n",
    "    # df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    # df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    # df.drop('month', axis=1, inplace=True)\n",
    "    df.drop('created_at', axis=1, inplace=True)\n",
    "    df['steward'].fillna('0', inplace=True)\n",
    "    df['guards'].fillna('0', inplace=True)\n",
    "    df['problems'].fillna('NoProblem', inplace=True)\n",
    "    df['spc_genus'] = df['spc_latin'].str.split(' ').str[0]\n",
    "    df.drop('spc_latin', axis=1, inplace=True)\n",
    "\n",
    "    # df['curb_loc'] = df['curb_loc'].map({'OnCurb':3, 'OffsetFromCurb':1})\n",
    "    # df['sidewalk'] = df['sidewalk'].map({'NoDamage':1, 'Damage':3})\n",
    "    # df['guards'] = df['guards'].map({'Helpful':1, 'Harmful':3, '0':2, 'unsure':2})\n",
    "    # df['status_point'] = df['curb_loc'] * df['sidewalk'] * df['guards']\n",
    "    return df\n",
    "\n",
    "train_clean = cleansing(train_df)\n",
    "test_clean = cleansing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koba_\\AppData\\Local\\Temp\\ipykernel_10976\\1467932340.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_clean.drop('health', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 種目ごとの平均直径とその差を特徴量にする\n",
    "test_clean['health'] = 0\n",
    "all_df = pd.concat([train_clean, test_clean], axis=0)   \n",
    "tmp_df = all_df[['tree_dbh', 'spc_common', ]]\n",
    "dbh_mean_common = (pd.DataFrame(tmp_df.groupby('spc_common').mean()['tree_dbh']))\n",
    "dbh_mean_common.rename(columns={'tree_dbh': 'dbh_mean_common'}, inplace=True)\n",
    "all_df = pd.merge(all_df, dbh_mean_common, on='spc_common', how='left')\n",
    "# all_df['dbh_diff_common'] = all_df['tree_dbh'] - all_df['dbh_mean_common']\n",
    "\n",
    "tmp_df = all_df[['tree_dbh', 'cb_num']]\n",
    "dbh_mean_cb = (pd.DataFrame(tmp_df.groupby('cb_num').mean()['tree_dbh']))\n",
    "dbh_mean_cb.rename(columns={'tree_dbh': 'dbh_mean_cb'}, inplace=True)\n",
    "all_df = pd.merge(all_df, dbh_mean_cb, on='cb_num', how='left')\n",
    "\n",
    "# all_df['diff'] = all_df['dbh_mean_common'] - all_df['tree_dbh']\n",
    "\n",
    "train_clean = all_df.iloc[:len(train_clean)]\n",
    "test_clean = all_df.iloc[len(train_clean):]\n",
    "test_clean.drop('health', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# trainとtestに含まれるproblemは全く同じ\\n# problemのonehotカラムを用意\\nproblem_list = ['BranchLights', 'BranchOther', 'MetalGrates', 'RootOther', 'Stones', 'Sneakers', 'TrunkLights',\\n                 'TrunkOther', 'WiresRope', 'NoProblem']\\t\\n\\nfor problem in problem_list:\\n    train_clean.loc[:, problem] = 0\\n    test_clean.loc[:, problem] = 0\\n\\ntrain_clean['problem_count'] = 0\\ntest_clean['problem_count'] = 0\\n\\n# problemlistにあったらonehotする\\nfor i in train_clean.index:\\n    p_count = 0\\n    for problem in problem_list:\\n        if(problem in train_clean.loc[i, 'problems']):\\n            train_clean.loc[i, problem] = 1\\n            if(problem != 'NoProblem'):\\n                p_count+=1\\n    train_clean.loc[i, 'problem_count'] = p_count\\n\\nfor i in test_clean.index:\\n    p_count = 0\\n    for problem in problem_list:\\n        if(problem in test_clean.loc[i, 'problems']):\\n            test_clean.loc[i, problem] = 1\\n            if(problem != 'NoProblem'):\\n                p_count+=1\\n    test_clean.loc[i, 'problem_count'] = p_count\\n\\ntest_clean.head()\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# trainとtestに含まれるproblemは全く同じ\n",
    "# problemのonehotカラムを用意\n",
    "problem_list = ['BranchLights', 'BranchOther', 'MetalGrates', 'RootOther', 'Stones', 'Sneakers', 'TrunkLights',\n",
    "                 'TrunkOther', 'WiresRope', 'NoProblem']\t\n",
    "\n",
    "for problem in problem_list:\n",
    "    train_clean.loc[:, problem] = 0\n",
    "    test_clean.loc[:, problem] = 0\n",
    "\n",
    "train_clean['problem_count'] = 0\n",
    "test_clean['problem_count'] = 0\n",
    "\n",
    "# problemlistにあったらonehotする\n",
    "for i in train_clean.index:\n",
    "    p_count = 0\n",
    "    for problem in problem_list:\n",
    "        if(problem in train_clean.loc[i, 'problems']):\n",
    "            train_clean.loc[i, problem] = 1\n",
    "            if(problem != 'NoProblem'):\n",
    "                p_count+=1\n",
    "    train_clean.loc[i, 'problem_count'] = p_count\n",
    "\n",
    "for i in test_clean.index:\n",
    "    p_count = 0\n",
    "    for problem in problem_list:\n",
    "        if(problem in test_clean.loc[i, 'problems']):\n",
    "            test_clean.loc[i, problem] = 1\n",
    "            if(problem != 'NoProblem'):\n",
    "                p_count+=1\n",
    "    test_clean.loc[i, 'problem_count'] = p_count\n",
    "\n",
    "test_clean.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## やりたい実験\n",
    "- あらゆるエンコーディング方法の比較 ok\n",
    "- 地区ごとの予測モデルの実装 \n",
    "- 365日で一周するsin, cos 意味無し\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error! categorical is not same\n",
      "足りないカラム: {'year', 'month'}\n"
     ]
    }
   ],
   "source": [
    "# エンコーディングパート\n",
    "\n",
    "# すべてのカテゴリカル変数\n",
    "all_categorical_cols = ['curb_loc', 'steward', 'guards', 'sidewalk',\n",
    "       'user_type', 'problems', 'spc_common', 'nta', 'nta_name', 'borocode',\n",
    "       'boro_ct', 'boroname', 'zip_city', 'cb_num', 'st_senate', 'st_assem',\n",
    "       'cncldist', 'year', 'month',  'spc_genus']\n",
    "\n",
    "# \n",
    "\n",
    "# 落とすカラム\n",
    "drop_cols = ['nta_name', 'boroname']#'',\n",
    "# カウントエンコーディング\n",
    "ce_columns = ['curb_loc', 'steward', 'guards', 'sidewalk',\n",
    "       'user_type', 'problems', 'spc_common', 'nta', \n",
    "              'borocode', 'boro_ct',  'zip_city', 'cb_num', 'st_senate',\n",
    "       'st_assem', 'cncldist', 'spc_genus', ]\n",
    "# 'nta',\n",
    "#  \n",
    "# \n",
    "\n",
    "# ターゲットエンコーディング\n",
    "te_columns = []\n",
    "# ラベルエンコーディング\n",
    "le_columns = []\n",
    "encoding_cals = list(drop_cols + ce_columns + te_columns + drop_cols)\n",
    "if(encoding_cals != all_categorical_cols):\n",
    "    print('error! categorical is not same')\n",
    "    print('足りないカラム:', set(all_categorical_cols)-set(encoding_cals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットエンコーディングパート\n",
    "if(len(te_columns) > 0):\n",
    "    # onehot正解ラベルの作成\n",
    "    for i in range(3):\n",
    "        train_clean['onehot' + str(i)] = 0\n",
    "        train_clean['onehot' + str(i)] = train_clean['onehot' + str(i)].mask(train_clean['health'] == i, 1)\n",
    "\n",
    "    # ターゲットエンコーディングのカラムを3つずつに分ける\n",
    "    te_columns_list = []\n",
    "\n",
    "    for te_column in te_columns:\n",
    "        tmp_list = []\n",
    "        for i in range(3):\n",
    "            train_clean[te_column + '-te' + str(i)] = train_clean[te_column]\n",
    "            test_clean[te_column + '-te' + str(i)] = test_clean[te_column]\n",
    "            tmp_list.append(te_column + '-te' + str(i))\n",
    "        te_columns_list.append(tmp_list)\n",
    "        train_clean.drop(te_column, axis=1, inplace=True)\n",
    "        test_clean.drop(te_column, axis=1, inplace=True)\n",
    "\n",
    "    te_columns_list = np.array(te_columns_list)\n",
    "\n",
    "    # ターゲットエンコーディング\n",
    "    for i in range(3):\n",
    "        te_target = te_columns_list[:, i]\n",
    "        target_enc = TargetEncoder(target_type='binary', random_state=42)\n",
    "        \n",
    "        train_clean[te_target] = target_enc.fit_transform(train_clean[te_target], train_clean['onehot' + str(i)])\n",
    "        # テストデータにtransform\n",
    "        test_clean[te_target]  = target_enc.transform(test_clean[te_target])\n",
    "\n",
    "    # onehot正解ラベルを削除\n",
    "    for i in range(3):\n",
    "        dropcol = 'onehot' + str(i)\n",
    "        train_clean.drop(dropcol, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# 目的変数を指定\n",
    "y = train_clean['health']\n",
    "\n",
    "# 使わないcolumnを削除\n",
    "train_clean_drop = train_clean.drop(['health'], axis=1)\n",
    "train_clean_drop = train_clean_drop.drop(drop_cols, axis=1)\n",
    "test_clean_drop = test_clean.drop(drop_cols,axis=1)\n",
    "\n",
    "# ラベルエンコーディングパート\n",
    "le = LabelEncoder()\n",
    "for column in le_columns:\n",
    "    train_clean_drop[column] = le.fit_transform(train_clean_drop[column])\n",
    "    test_clean_drop[column] = le.transform(test_clean_drop[column])\n",
    "\n",
    "# カテゴリカル変数を指定\n",
    "for col in le_columns:\n",
    "    train_clean_drop[col] = train_clean_drop[col].astype('category')\n",
    "    test_clean_drop[col] = test_clean_drop[col].astype('category')  \n",
    "\n",
    "# カウントエンコーディングパート\n",
    "# mergeしてfit, そのあとtransform\n",
    "all_df = pd.concat([train_clean_drop, test_clean_drop], axis=0)\n",
    "\n",
    "# count encoding\n",
    "cencoder = ce.CountEncoder(cols=ce_columns)\n",
    "cencoder.fit(all_df)\n",
    "\n",
    "# count encodingを適用\n",
    "train_clean_drop = cencoder.transform(train_clean_drop)\n",
    "test_clean_drop = cencoder.transform(test_clean_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0 start\n",
      "[0]\tvalidation_0-logloss:0.66629\tvalidation_1-logloss:0.66696\n",
      "[100]\tvalidation_0-logloss:0.13910\tvalidation_1-logloss:0.26894\n",
      "[200]\tvalidation_0-logloss:0.04580\tvalidation_1-logloss:0.23935\n",
      "[244]\tvalidation_0-logloss:0.03189\tvalidation_1-logloss:0.24737\n",
      "fold0 f1_score: 0.5031\n",
      "[0]\tvalidation_0-logloss:0.68036\tvalidation_1-logloss:0.68811\n",
      "[100]\tvalidation_0-logloss:0.41320\tvalidation_1-logloss:0.63616\n",
      "[192]\tvalidation_0-logloss:0.29082\tvalidation_1-logloss:0.63127\n",
      "fold0 f1_score: 0.5166\n",
      "fold1 start\n",
      "[0]\tvalidation_0-logloss:0.65232\tvalidation_1-logloss:0.65794\n",
      "[100]\tvalidation_0-logloss:0.12968\tvalidation_1-logloss:0.25682\n",
      "[200]\tvalidation_0-logloss:0.04451\tvalidation_1-logloss:0.23152\n",
      "[229]\tvalidation_0-logloss:0.03438\tvalidation_1-logloss:0.23396\n",
      "fold1 f1_score: 0.5192\n",
      "[0]\tvalidation_0-logloss:0.68327\tvalidation_1-logloss:0.68858\n",
      "[100]\tvalidation_0-logloss:0.40062\tvalidation_1-logloss:0.61994\n",
      "[166]\tvalidation_0-logloss:0.31319\tvalidation_1-logloss:0.62003\n",
      "fold1 f1_score: 0.5220\n",
      "fold2 start\n",
      "[0]\tvalidation_0-logloss:0.65314\tvalidation_1-logloss:0.66253\n",
      "[100]\tvalidation_0-logloss:0.12926\tvalidation_1-logloss:0.25330\n",
      "[200]\tvalidation_0-logloss:0.04493\tvalidation_1-logloss:0.22910\n",
      "[232]\tvalidation_0-logloss:0.03433\tvalidation_1-logloss:0.23132\n",
      "fold2 f1_score: 0.4984\n",
      "[0]\tvalidation_0-logloss:0.68035\tvalidation_1-logloss:0.68601\n",
      "[100]\tvalidation_0-logloss:0.41834\tvalidation_1-logloss:0.62620\n",
      "[200]\tvalidation_0-logloss:0.29145\tvalidation_1-logloss:0.62080\n",
      "[203]\tvalidation_0-logloss:0.28876\tvalidation_1-logloss:0.62164\n",
      "fold2 f1_score: 0.5260\n",
      "fold3 start\n",
      "[0]\tvalidation_0-logloss:0.65843\tvalidation_1-logloss:0.66470\n",
      "[100]\tvalidation_0-logloss:0.12627\tvalidation_1-logloss:0.26690\n",
      "[200]\tvalidation_0-logloss:0.04722\tvalidation_1-logloss:0.24211\n",
      "[235]\tvalidation_0-logloss:0.03573\tvalidation_1-logloss:0.24408\n",
      "fold3 f1_score: 0.5282\n",
      "[0]\tvalidation_0-logloss:0.68056\tvalidation_1-logloss:0.68940\n",
      "[100]\tvalidation_0-logloss:0.40847\tvalidation_1-logloss:0.63036\n",
      "[176]\tvalidation_0-logloss:0.31335\tvalidation_1-logloss:0.63158\n",
      "fold3 f1_score: 0.5126\n",
      "fold4 start\n",
      "[0]\tvalidation_0-logloss:0.66251\tvalidation_1-logloss:0.66659\n",
      "[100]\tvalidation_0-logloss:0.12653\tvalidation_1-logloss:0.24974\n",
      "[200]\tvalidation_0-logloss:0.04424\tvalidation_1-logloss:0.22391\n",
      "[244]\tvalidation_0-logloss:0.02978\tvalidation_1-logloss:0.22621\n",
      "fold4 f1_score: 0.5236\n",
      "[0]\tvalidation_0-logloss:0.68055\tvalidation_1-logloss:0.68533\n",
      "[100]\tvalidation_0-logloss:0.41884\tvalidation_1-logloss:0.61600\n",
      "[200]\tvalidation_0-logloss:0.29387\tvalidation_1-logloss:0.61274\n",
      "[240]\tvalidation_0-logloss:0.25652\tvalidation_1-logloss:0.61483\n",
      "fold4 f1_score: 0.5057\n",
      "first-CV: 0.5145\n",
      "second-CV: 0.5166\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([train_clean_drop, y], axis=1)\n",
    "origin_df = pd.read_csv('C:/python/signate/data/train.csv', index_col=0)\n",
    "group_num_big = 1\n",
    "group_num_small = 0\n",
    "solo_num = 2\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "first_valid_scores = []\n",
    "second_valid_scores = []\n",
    "first_models = []\n",
    "second_models = []\n",
    "# train_dfと同じindexを持つデータフレームを作成\n",
    "lgb_pred_proba = origin_df[['health']].copy()\n",
    "lgb_pred_proba.rename(columns={'health':'xgb'}, inplace=True)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df, y)):\n",
    "    print(f'fold{fold} start')\n",
    "\n",
    "    # 0,2と1の予測モデル\n",
    "    X_train = train_df.iloc[train_idx]\n",
    "    X_valid = train_df.iloc[valid_idx]\n",
    "\n",
    "    # 元々のyを保持\n",
    "    y_all_train = X_train['health']\n",
    "    y_all_valid = X_valid['health']\n",
    "\n",
    "    # 0,2のhelathを0とする\n",
    "    X_train_first = X_train.copy()\n",
    "    X_valid_first = X_valid.copy()\n",
    "\n",
    "    # groupは0,soloは1にする\n",
    "    X_train_first['health'] = X_train_first['health'].mask(X_train_first['health'] == group_num_big, 1)\n",
    "    X_valid_first['health'] = X_valid_first['health'].mask(X_valid_first['health'] == group_num_big, 1)\n",
    "    X_train_first['health'] = X_train_first['health'].mask(X_train_first['health'] == group_num_small, 1)\n",
    "    X_valid_first['health'] = X_valid_first['health'].mask(X_valid_first['health'] == group_num_small, 1)\n",
    "    X_train_first['health'] = X_train_first['health'].mask(X_train_first['health'] == solo_num, 0)\n",
    "    X_valid_first['health'] = X_valid_first['health'].mask(X_valid_first['health'] == solo_num, 0)\n",
    "\n",
    "    # yを分離\n",
    "    y_train_first = X_train_first['health']\n",
    "    X_train_first_drop = X_train_first.drop('health', axis=1)\n",
    "    y_valid_first = X_valid_first['health']\n",
    "    X_valid_first_drop = X_valid_first.drop('health', axis=1)\n",
    "\n",
    "    # 1段目の予測\n",
    "\n",
    "    verbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\n",
    "    params = {'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss', 'seed': 0, 'n_estimators':10000, \n",
    "    'random_state':0\n",
    "    }\n",
    "    \n",
    "    first_model = XGBClassifier(**params, early_stopping_rounds=50)\n",
    "    first_model.fit(X_train_first_drop, y_train_first, \n",
    "                    eval_set=[(X_train_first_drop, y_train_first), (X_valid_first_drop, y_valid_first)], \n",
    "                    verbose=100, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train_first).astype('float32'),)\n",
    "\n",
    "    y_pred = first_model.predict(X_valid_first_drop,)\n",
    "    # lgb_pred_proba.iloc[valid_idx] = y_pred\n",
    "    # 予測結果の2値化\n",
    "    y_pred_max = np.where(y_pred < 0.5, 0, 1)\n",
    "    score = f1_score(y_valid_first, y_pred_max, average='macro')\n",
    "    print(f'fold{fold} f1_score: {score:.4f}')\n",
    "    first_valid_scores.append(score)\n",
    "    first_models.append(first_model)\n",
    "\n",
    "    # 0と2の予測\n",
    "    X_train_second = X_train.copy()\n",
    "    X_valid_second = X_valid.copy()\n",
    "    # groupnumを抽出\n",
    "    X_train_second = X_train_second[X_train_second['health'] != solo_num]\n",
    "    X_valid_second = X_valid_second[X_valid_second['health'] != solo_num]\n",
    "\n",
    "    # yをmask\n",
    "    X_train_second['health'] = X_train_second['health'].mask(X_train_second['health'] == group_num_big, 1)\n",
    "    X_train_second['health'] = X_train_second['health'].mask(X_train_second['health'] == group_num_small, 0)\n",
    "    X_valid_second['health'] = X_valid_second['health'].mask(X_valid_second['health'] == group_num_big, 1)\n",
    "    X_valid_second['health'] = X_valid_second['health'].mask(X_valid_second['health'] == group_num_small, 0)\n",
    "\n",
    "    # yを分離\n",
    "    y_train_second = X_train_second['health']\n",
    "    X_train_second_drop = X_train_second.drop('health', axis=1)\n",
    "    y_valid_second = X_valid_second['health']\n",
    "    X_valid_second_drop = X_valid_second.drop('health', axis=1, )\n",
    "\n",
    "\n",
    "    second_model = XGBClassifier(**params, early_stopping_rounds=50)\n",
    "    second_model.fit(X_train_second_drop, y_train_second, eval_set=[(X_train_second_drop, y_train_second), (X_valid_second_drop, y_valid_second)], \n",
    "                    verbose=100, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train_second).astype('float32'),)\n",
    "\n",
    "    y_pred = second_model.predict(X_valid_second_drop,)\n",
    "    # lgb_pred_proba.iloc[valid_idx] = y_pred\n",
    "    y_pred_max = np.where(y_pred<0.5, 0, 1)\n",
    "    score = f1_score(y_valid_second, y_pred_max, average='macro')\n",
    "    print(f'fold{fold} f1_score: {score:.4f}')\n",
    "    second_valid_scores.append(score)\n",
    "    second_models.append(second_model)\n",
    "print(f'first-CV: {np.mean(first_valid_scores):.4f}')\n",
    "print(f'second-CV: {np.mean(second_valid_scores):.4f}')\n",
    "# lgb_pred_proba.to_csv('C:/python/signate/data/lgb_pred_proba.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0 f1_score: 0.3477\n",
      "fold1 f1_score: 0.3591\n",
      "fold2 f1_score: 0.3495\n",
      "fold3 f1_score: 0.3603\n",
      "fold4 f1_score: 0.3527\n",
      "CV: 0.3539\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   xgb\n",
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルで評価\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "second_valid_scores = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df, y)):\n",
    "    X_train = train_df.iloc[train_idx]\n",
    "    X_valid = train_df.iloc[valid_idx]\n",
    "    y_valid = X_valid['health']\n",
    "    X_valid_first = X_valid.copy()\n",
    "    X_valid_first.drop('health', axis=1, inplace=True)\n",
    "\n",
    "    # 1段目の予測\n",
    "    first_model = first_models[fold]\n",
    "    y_pred_first = first_model.predict(X_valid_first,)\n",
    "    # 予測結果の2値化\n",
    "    y_pred_first = np.where(y_pred_first < 0.5, solo_num, 1)\n",
    "    \n",
    "    # 0, 2の予測\n",
    "    # 1段目の予測結果を結合\n",
    "    X_valid_second = pd.concat([X_valid_first, pd.Series(y_pred_first, index=X_valid_first.index)], axis=1)\n",
    "    # 使うのは1段目で0と予測したデータ    \n",
    "    X_valid_second_target = X_valid_second[X_valid_second[0] == 1]\n",
    "    X_valid_second_drop = X_valid_second_target.drop(0, axis=1)\n",
    "    \n",
    "    # 2段目の予測\n",
    "    second_model = second_models[fold]\n",
    "    y_pred_second = second_model.predict(X_valid_second_drop, )\n",
    "    # 予測結果の2値化は2と0\n",
    "    y_pred_second = np.where(y_pred_second > 0.5, group_num_big, group_num_small)\n",
    "    \n",
    "    # 1段目で0と予測したデータに2段目の予測結果を代入\n",
    "    y_pred = pd.Series(y_pred_first, index=X_valid_first.index)\n",
    "    y_pred[y_pred == 1] = y_pred_second\n",
    "    score = f1_score(y_valid, y_pred, average='macro')\n",
    "    print(f'fold{fold} f1_score: {score:.4f}')\n",
    "    lgb_pred_proba.loc[valid_idx, 'xgb'] = y_pred\n",
    "    second_valid_scores.append(score)\n",
    "    \n",
    "print(f'CV: {np.mean(second_valid_scores):.4f}')\n",
    "lgb_pred_proba.to_csv('C:/python/signate/data/xgb_dive-train.csv')\n",
    "lgb_pred_proba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19984 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       xgb\n",
       "0        0\n",
       "1        1\n",
       "2        0\n",
       "3        1\n",
       "4        1\n",
       "...    ...\n",
       "19979    1\n",
       "19980    1\n",
       "19981    1\n",
       "19982    1\n",
       "19983    1\n",
       "\n",
       "[19984 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータデータで予測\n",
    "# 1段目の予測\n",
    "first_model = first_models[4]\n",
    "y_pred_first = first_model.predict(test_clean_drop,)\n",
    "# 予測結果の2値化\n",
    "y_pred_first = np.where(y_pred_first < 0.5, solo_num, 1)\n",
    "\n",
    "# 0, 2の予測\n",
    "# 1段目の予測結果を結合\n",
    "test_second = pd.concat([test_clean_drop, pd.Series(y_pred_first, index=test_clean_drop.index)], axis=1)\n",
    "# 使うのは1段目で0と予測したデータ\n",
    "test_second_target = test_second[test_second[0] == 1]\n",
    "test_second_drop = test_second_target.drop(0, axis=1)\n",
    "# 2段目の予測\n",
    "second_model = second_models[4]\n",
    "y_pred_second = second_model.predict(test_second_drop)\n",
    "# 予測結果の2値化は2と0\n",
    "y_pred_second_th = np.where(y_pred_second > 0.5, group_num_big, group_num_small)\n",
    "\n",
    "# 1段目で0と予測したデータに2段目の予測結果を代入\n",
    "y_pred_series = pd.Series(y_pred_first, index=test_clean_drop.index)\n",
    "y_pred_series_copy = y_pred_series.copy()\n",
    "y_pred_series_copy[y_pred_series_copy == 1] = y_pred_second_th\n",
    "y_pred_series_copy.index = sample_df.index\n",
    "y_pred_series_copy.to_csv('C:/python/signate/data/xgb-dive.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlgb_train = lgb.Dataset(train_clean_drop, y, weight=compute_sample_weight(class_weight='balanced', y=y).astype('float32'))\\nverbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\\nparams = {'objective': 'multiclass',\\n'metric': 'multi_logloss',\\n'num_class': 3,\\n'seed': 0\\n}\\n\\nmodel = lgb.train(params,\\n                    lgb_train,\\n                    num_boost_round=520,\\n                )\\n\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全データを利用して学習\n",
    "'''\n",
    "lgb_train = lgb.Dataset(train_clean_drop, y, weight=compute_sample_weight(class_weight='balanced', y=y).astype('float32'))\n",
    "verbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\n",
    "params = {'objective': 'multiclass',\n",
    "'metric': 'multi_logloss',\n",
    "'num_class': 3,\n",
    "'seed': 0\n",
    "}\n",
    "\n",
    "model = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=520,\n",
    "                )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m lgb_pred_proba_test \u001b[38;5;241m=\u001b[39m test_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborocode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree_dbh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnta\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      2\u001b[0m lgb_pred_proba_test\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborocode\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb-0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree_dbh\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb-1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnta\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb-2\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(test_clean_drop)\n\u001b[0;32m      5\u001b[0m lgb_pred_proba_test\u001b[38;5;241m.\u001b[39miloc[:] \u001b[38;5;241m=\u001b[39m y_pred[:]\n\u001b[0;32m      6\u001b[0m lgb_pred_proba_test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/python/signate/data/lgb_pred_proba_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "lgb_pred_proba_test = test_df[['borocode', 'tree_dbh', 'nta']].copy()\n",
    "lgb_pred_proba_test.rename(columns= {'borocode':'lgb-0', 'tree_dbh':'lgb-1', 'nta':'lgb-2'}, inplace=True)\n",
    "\n",
    "y_pred = model.predict(test_clean_drop)\n",
    "lgb_pred_proba_test.iloc[:] = y_pred[:]\n",
    "lgb_pred_proba_test.to_csv('C:/python/signate/data/lgb_pred_proba_test.csv')\n",
    "y_pred_max = np.argmax(y_pred, axis=1) # 最尤と判断したクラスの値にする\n",
    "sample_df[1] = y_pred_max\n",
    "# sample_df.to_csv('C:/python/signate/data/base3(cv0.3586).csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tree_dbh</td>\n",
       "      <td>1763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>boro_ct</td>\n",
       "      <td>1692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nta</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spc_common</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>problems</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dbh_mean_common</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spc_genus</td>\n",
       "      <td>853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>month_cos</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>st_assem</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>month_sin</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dbh_mean_cb</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cb_num</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cncldist</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zip_city</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>st_senate</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user_type</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guards</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steward</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sidewalk</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>curb_loc</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>borocode</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature  Importance\n",
       "0          tree_dbh        1763\n",
       "10          boro_ct        1692\n",
       "8               nta        1177\n",
       "7        spc_common        1094\n",
       "6          problems         990\n",
       "19  dbh_mean_common         880\n",
       "18        spc_genus         853\n",
       "17        month_cos         752\n",
       "14         st_assem         723\n",
       "16        month_sin         683\n",
       "20      dbh_mean_cb         651\n",
       "12           cb_num         587\n",
       "15         cncldist         547\n",
       "11         zip_city         516\n",
       "13        st_senate         499\n",
       "5         user_type         440\n",
       "3            guards         322\n",
       "2           steward         300\n",
       "4          sidewalk         186\n",
       "1          curb_loc         155\n",
       "9          borocode         100"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importance = first_model.feature_importance()\n",
    "feature_names = first_model.feature_name()\n",
    "\n",
    "df_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "# importtane順に並び替え\n",
    "df_importance = df_importance.sort_values('Importance', ascending=False)\n",
    "df_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
