{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, TargetEncoder\n",
    "# import optuna.integration.lightgbm as lgb\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import category_encoders as ce\n",
    "import math\n",
    "\n",
    "# pandasの行を省略しない\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/python/signate/data/train.csv', index_col=0)\n",
    "test_df = pd.read_csv('C:/python/signate/data/test.csv', index_col=0)\n",
    "sample_df = pd.read_csv('C:/python/signate/data/sample_submission.csv', header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_dbh</th>\n",
       "      <th>curb_loc</th>\n",
       "      <th>health</th>\n",
       "      <th>steward</th>\n",
       "      <th>guards</th>\n",
       "      <th>sidewalk</th>\n",
       "      <th>user_type</th>\n",
       "      <th>problems</th>\n",
       "      <th>spc_common</th>\n",
       "      <th>nta</th>\n",
       "      <th>nta_name</th>\n",
       "      <th>borocode</th>\n",
       "      <th>boro_ct</th>\n",
       "      <th>boroname</th>\n",
       "      <th>zip_city</th>\n",
       "      <th>cb_num</th>\n",
       "      <th>st_senate</th>\n",
       "      <th>st_assem</th>\n",
       "      <th>cncldist</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>spc_genus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Damage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>NoProblem</td>\n",
       "      <td>English oak</td>\n",
       "      <td>QN45</td>\n",
       "      <td>Douglas Manor-Douglaston-Little Neck</td>\n",
       "      <td>4</td>\n",
       "      <td>4152901</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Little Neck</td>\n",
       "      <td>411</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>Quercus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>1</td>\n",
       "      <td>3or4</td>\n",
       "      <td>Helpful</td>\n",
       "      <td>NoDamage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>NoProblem</td>\n",
       "      <td>crimson king maple</td>\n",
       "      <td>BX05</td>\n",
       "      <td>Bedford Park-Fordham North</td>\n",
       "      <td>2</td>\n",
       "      <td>2039901</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>207</td>\n",
       "      <td>33</td>\n",
       "      <td>78</td>\n",
       "      <td>15</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>Acer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NoDamage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>StonesBranchLights</td>\n",
       "      <td>English oak</td>\n",
       "      <td>SI01</td>\n",
       "      <td>Annadale-Huguenot-Prince's Bay-Eltingville</td>\n",
       "      <td>5</td>\n",
       "      <td>5017011</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>503</td>\n",
       "      <td>24</td>\n",
       "      <td>62</td>\n",
       "      <td>51</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>Quercus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Damage</td>\n",
       "      <td>NYC Parks Staff</td>\n",
       "      <td>NoProblem</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>SI11</td>\n",
       "      <td>Charleston-Richmond Valley-Tottenville</td>\n",
       "      <td>5</td>\n",
       "      <td>5024401</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>503</td>\n",
       "      <td>24</td>\n",
       "      <td>62</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>Gleditsia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NoDamage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>Stones</td>\n",
       "      <td>London planetree</td>\n",
       "      <td>MN03</td>\n",
       "      <td>Central Harlem North-Polo Grounds</td>\n",
       "      <td>1</td>\n",
       "      <td>1022102</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>New York</td>\n",
       "      <td>110</td>\n",
       "      <td>30</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>Platanus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tree_dbh curb_loc  health steward   guards  sidewalk        user_type  \\\n",
       "0        14   OnCurb       1       0        0    Damage        Volunteer   \n",
       "1         5   OnCurb       1    3or4  Helpful  NoDamage        Volunteer   \n",
       "2        26   OnCurb       2       0        0  NoDamage        Volunteer   \n",
       "3        15   OnCurb       0       0        0    Damage  NYC Parks Staff   \n",
       "4        23   OnCurb       1       0        0  NoDamage        Volunteer   \n",
       "\n",
       "             problems          spc_common   nta  \\\n",
       "0           NoProblem         English oak  QN45   \n",
       "1           NoProblem  crimson king maple  BX05   \n",
       "2  StonesBranchLights         English oak  SI01   \n",
       "3           NoProblem         honeylocust  SI11   \n",
       "4              Stones    London planetree  MN03   \n",
       "\n",
       "                                     nta_name  borocode  boro_ct  \\\n",
       "0        Douglas Manor-Douglaston-Little Neck         4  4152901   \n",
       "1                  Bedford Park-Fordham North         2  2039901   \n",
       "2  Annadale-Huguenot-Prince's Bay-Eltingville         5  5017011   \n",
       "3      Charleston-Richmond Valley-Tottenville         5  5024401   \n",
       "4           Central Harlem North-Polo Grounds         1  1022102   \n",
       "\n",
       "        boroname       zip_city  cb_num  st_senate  st_assem  cncldist  year  \\\n",
       "0         Queens    Little Neck     411         11        25        23  2015   \n",
       "1          Bronx          Bronx     207         33        78        15  2016   \n",
       "2  Staten Island  Staten Island     503         24        62        51  2015   \n",
       "3  Staten Island  Staten Island     503         24        62        51  2016   \n",
       "4      Manhattan       New York     110         30        70         9  2016   \n",
       "\n",
       "   month  spc_genus  \n",
       "0      6    Quercus  \n",
       "1      9       Acer  \n",
       "2      9    Quercus  \n",
       "3      5  Gleditsia  \n",
       "4      6   Platanus  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleansing(df):\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    # df['cos_day'] = df['created_at'].dt.dayofyear\n",
    "    # df['cos_day'] = df['cos_day'].apply(lambda x: np.cos(math.radians(90 - (x/365)*365)))\n",
    "    # df['sin_day'] = df['created_at'].dt.dayofyear\n",
    "    # df['sin_day'] = df['sin_day'].apply(lambda x: np.sin(math.radians(90 - (x/365)*365)))  \n",
    "    df['year'] = df['created_at'].dt.year\n",
    "    df['month'] = df['created_at'].dt.month\n",
    "    # df['weekday'] = df['created_at'].dt.weekday\n",
    "    # df['day'] = df.created_at.dt.day\n",
    "    # df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    # df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df.drop('created_at', axis=1, inplace=True)\n",
    "    df['steward'].fillna('0', inplace=True)\n",
    "    df['guards'].fillna('0', inplace=True)\n",
    "    df['problems'].fillna('NoProblem', inplace=True)\n",
    "    df['spc_genus'] = df['spc_latin'].str.split(' ').str[0]\n",
    "    df.drop('spc_latin', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # df['curb_loc'] = df['curb_loc'].map({'OnCurb':3, 'OffsetFromCurb':1})\n",
    "    # df['sidewalk'] = df['sidewalk'].map({'NoDamage':1, 'Damage':3})\n",
    "    # df['guards'] = df['guards'].map({'Helpful':1, 'Harmful':3, '0':2, 'unsure':2})\n",
    "    # df['status_point'] = df['curb_loc'] * df['sidewalk'] * df['guards']\n",
    "    return df\n",
    "\n",
    "train_clean = cleansing(train_df)\n",
    "test_clean = cleansing(test_df)\n",
    "train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 種目ごとの平均直径とその差を特徴量にする\\ntest_clean['health'] = 0\\nall_df = pd.concat([train_clean, test_clean], axis=0)   \\ntmp_df = all_df[['tree_dbh', 'spc_common', ]]\\ndbh_mean_common = (pd.DataFrame(tmp_df.groupby('spc_common').mean()['tree_dbh']))\\ndbh_mean_common.rename(columns={'tree_dbh': 'dbh_mean_common'}, inplace=True)\\nall_df = pd.merge(all_df, dbh_mean_common, on='spc_common', how='left')\\nall_df['dbh_diff_common'] = all_df['tree_dbh'] - all_df['dbh_mean_common']\\n\\ntmp_df = all_df[['tree_dbh', 'spc_genus', ]]\\ndbh_mean_genus = (pd.DataFrame(tmp_df.groupby('spc_genus').mean()['tree_dbh']))\\ndbh_mean_genus.rename(columns={'tree_dbh': 'dbh_mean_genus'}, inplace=True)\\nall_df = pd.merge(all_df, dbh_mean_genus, on='spc_genus', how='left')\\nall_df['dbh_diff_genus'] = all_df['tree_dbh'] - all_df['dbh_mean_genus']\\n\\ntrain_clean = all_df.iloc[:len(train_clean)]\\ntest_clean = all_df.iloc[len(train_clean):]\\ntrain_clean = train_clean.drop(['dbh_mean_common', 'tree_dbh'], axis=1, )\\ntest_clean = test_clean.drop(['health', 'dbh_mean_common', 'tree_dbh'], axis=1, )\\ntest_clean.head()\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 種目ごとの平均直径とその差を特徴量にする\n",
    "test_clean['health'] = 0\n",
    "all_df = pd.concat([train_clean, test_clean], axis=0)   \n",
    "tmp_df = all_df[['tree_dbh', 'spc_common', ]]\n",
    "dbh_mean_common = (pd.DataFrame(tmp_df.groupby('spc_common').mean()['tree_dbh']))\n",
    "dbh_mean_common.rename(columns={'tree_dbh': 'dbh_mean_common'}, inplace=True)\n",
    "all_df = pd.merge(all_df, dbh_mean_common, on='spc_common', how='left')\n",
    "all_df['dbh_diff_common'] = all_df['tree_dbh'] - all_df['dbh_mean_common']\n",
    "\n",
    "tmp_df = all_df[['tree_dbh', 'spc_genus', ]]\n",
    "dbh_mean_genus = (pd.DataFrame(tmp_df.groupby('spc_genus').mean()['tree_dbh']))\n",
    "dbh_mean_genus.rename(columns={'tree_dbh': 'dbh_mean_genus'}, inplace=True)\n",
    "all_df = pd.merge(all_df, dbh_mean_genus, on='spc_genus', how='left')\n",
    "all_df['dbh_diff_genus'] = all_df['tree_dbh'] - all_df['dbh_mean_genus']\n",
    "\n",
    "train_clean = all_df.iloc[:len(train_clean)]\n",
    "test_clean = all_df.iloc[len(train_clean):]\n",
    "train_clean = train_clean.drop(['dbh_mean_common', 'tree_dbh'], axis=1, )\n",
    "test_clean = test_clean.drop(['health', 'dbh_mean_common', 'tree_dbh'], axis=1, )\n",
    "test_clean.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カウントエンコーディングパート\n",
    "# すべてのカテゴリカル変数\n",
    "all_categorical_cols = ['curb_loc', 'steward', 'guards', 'sidewalk',\n",
    "    'user_type', 'problems', 'spc_common', 'nta', 'nta_name', 'borocode',\n",
    "    'boro_ct', 'boroname', 'zip_city', 'cb_num', 'st_senate', 'st_assem',\n",
    "    'cncldist', 'year', 'month',  'spc_genus', ]\n",
    "\n",
    "# 一度yを分離する\n",
    "train_y = train_clean['health']\n",
    "train_X = train_clean.drop('health', axis=1)\n",
    "\n",
    "# mergeしてfit, そのあとtransform\n",
    "all_df = pd.concat([train_X, test_clean], axis=0)\n",
    "\n",
    "# count encoding\n",
    "cencoder = ce.CountEncoder(cols=all_categorical_cols)\n",
    "cencoder.fit(all_df)\n",
    "\n",
    "# count encodingを適用\n",
    "train_clean_drop = cencoder.transform(train_X)\n",
    "test_clean_drop = cencoder.transform(test_clean)\n",
    "\n",
    "# yを再結合\n",
    "train_clean_drop = pd.concat([train_clean_drop, train_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:/python/signate/data/autogluon/\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"C:/python/signate/data/autogluon/\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          8\n",
      "Memory Avail:       1.42 GB / 7.85 GB (18.1%)\n",
      "Disk Space Avail:   212.41 GB / 475.69 GB (44.7%)\n",
      "===================================================\n",
      "Train Data Rows:    19984\n",
      "Train Data Columns: 21\n",
      "Label Column:       health\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\t3 unique label values:  [1, 2, 0]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1451.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.20 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['nta_name', 'boroname']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 2 | ['nta_name', 'boroname']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 19 | ['tree_dbh', 'curb_loc', 'steward', 'guards', 'sidewalk', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', [])       : 16 | ['tree_dbh', 'steward', 'guards', 'user_type', 'problems', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['curb_loc', 'sidewalk', 'year']\n",
      "\t0.6s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.50 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 17985, Val Rows: 1999\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.3331\t = Validation score   (f1_macro)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.3274\t = Validation score   (f1_macro)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t0.3403\t = Validation score   (f1_macro)\n",
      "\t20.86s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.2938\t = Validation score   (f1_macro)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.2938\t = Validation score   (f1_macro)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.2993\t = Validation score   (f1_macro)\n",
      "\t4.83s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 296 due to low memory. Expected memory usage reduced from 15.17% -> 15.0% of available memory...\n",
      "\t0.2961\t = Validation score   (f1_macro)\n",
      "\t5.52s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.2938\t = Validation score   (f1_macro)\n",
      "\t3.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 237 due to low memory. Expected memory usage reduced from 18.94% -> 15.0% of available memory...\n",
      "\t0.2977\t = Validation score   (f1_macro)\n",
      "\t2.48s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 208 due to low memory. Expected memory usage reduced from 21.63% -> 15.0% of available memory...\n",
      "\t0.2976\t = Validation score   (f1_macro)\n",
      "\t2.26s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.3054\t = Validation score   (f1_macro)\n",
      "\t7.98s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.2938\t = Validation score   (f1_macro)\n",
      "\t17.87s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.2938\t = Validation score   (f1_macro)\n",
      "\t3.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.818, 'KNeighborsUnif': 0.182}\n",
      "\t0.367\t = Validation score   (f1_macro)\n",
      "\t3.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 80.43s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:/python/signate/data/autogluon/\")\n"
     ]
    }
   ],
   "source": [
    "# train_df, valid_df = train_test_split(train_clean_drop, test_size=0.2, random_state=42, stratify=train_clean['health'])\n",
    "train_df = train_clean_drop\n",
    "dir_default = 'C:/python/signate/data/autogluon/'\n",
    "predictor = TabularPredictor(label='health', path=dir_default, eval_metric='f1_macro').fit(train_data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(test_clean_drop)\n",
    "sample_df[1] = y_pred\n",
    "sample_df.to_csv('C:/python/signate/data/autogluon.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.3405\n"
     ]
    }
   ],
   "source": [
    "valid_y = valid_df['health']\n",
    "valid_df.drop('health', axis=1, inplace=True)\n",
    "y_pred = predictor.predict(valid_df)\n",
    "score = f1_score(valid_y, y_pred, average='macro')\n",
    "print(f'f1_score: {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2   0.366965    f1_macro       0.329685  24.469586                0.002990           3.585828            2       True         14\n",
      "1       NeuralNetFastAI   0.340350    f1_macro       0.050864  20.864809                0.050864          20.864809            1       True          3\n",
      "2        KNeighborsUnif   0.333134    f1_macro       0.275831   0.018949                0.275831           0.018949            1       True          1\n",
      "3        KNeighborsDist   0.327425    f1_macro       0.142621   0.027923                0.142621           0.027923            1       True          2\n",
      "4               XGBoost   0.305437    f1_macro       0.089760   7.979094                0.089760           7.979094            1       True         11\n",
      "5      RandomForestGini   0.299280    f1_macro       0.158490   4.831115                0.158490           4.831115            1       True          6\n",
      "6        ExtraTreesGini   0.297697    f1_macro       0.144613   2.479174                0.144613           2.479174            1       True          9\n",
      "7        ExtraTreesEntr   0.297577    f1_macro       0.096435   2.256160                0.096435           2.256160            1       True         10\n",
      "8      RandomForestEntr   0.296057    f1_macro       0.142619   5.515244                0.142619           5.515244            1       True          7\n",
      "9            LightGBMXT   0.293788    f1_macro       0.009972   2.220586                0.009972           2.220586            1       True          4\n",
      "10             LightGBM   0.293788    f1_macro       0.009973   2.111539                0.009973           2.111539            1       True          5\n",
      "11        LightGBMLarge   0.293788    f1_macro       0.010936   3.601347                0.010936           3.601347            1       True         13\n",
      "12             CatBoost   0.293788    f1_macro       0.014687   3.577409                0.014687           3.577409            1       True          8\n",
      "13       NeuralNetTorch   0.293788    f1_macro       0.024932  17.871442                0.024932          17.871442            1       True         12\n",
      "Number of models trained: 14\n",
      "Types of models trained:\n",
      "{'RFModel', 'NNFastAiTabularModel', 'XGBoostModel', 'WeightedEnsembleModel', 'LGBModel', 'XTModel', 'TabularNeuralNetTorchModel', 'CatBoostModel', 'KNNModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('int', [])       : 16 | ['tree_dbh', 'steward', 'guards', 'user_type', 'problems', ...]\n",
      "('int', ['bool']) :  3 | ['curb_loc', 'sidewalk', 'year']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['nta_name', 'boroname']\n",
      "Computing feature importance via permutation shuffling for 19 features using 5000 rows with 5 shuffle sets...\n",
      "\t89.25s\t= Expected runtime (17.85s per shuffle set)\n",
      "\t43.32s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree_dbh</th>\n",
       "      <td>0.103443</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>1.544459e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.126215</td>\n",
       "      <td>0.080671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boro_ct</th>\n",
       "      <td>0.088029</td>\n",
       "      <td>0.009525</td>\n",
       "      <td>1.619399e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.107641</td>\n",
       "      <td>0.068417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spc_common</th>\n",
       "      <td>0.087082</td>\n",
       "      <td>0.014203</td>\n",
       "      <td>8.199706e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.116327</td>\n",
       "      <td>0.057837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nta</th>\n",
       "      <td>0.067274</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>2.732187e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.076857</td>\n",
       "      <td>0.057690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>st_assem</th>\n",
       "      <td>0.064464</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>3.784412e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.074430</td>\n",
       "      <td>0.054499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.062583</td>\n",
       "      <td>0.009903</td>\n",
       "      <td>7.278448e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.082974</td>\n",
       "      <td>0.042193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problems</th>\n",
       "      <td>0.061570</td>\n",
       "      <td>0.008490</td>\n",
       "      <td>4.229973e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.079050</td>\n",
       "      <td>0.044090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb_num</th>\n",
       "      <td>0.060440</td>\n",
       "      <td>0.009493</td>\n",
       "      <td>7.069079e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.079987</td>\n",
       "      <td>0.040894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spc_genus</th>\n",
       "      <td>0.057772</td>\n",
       "      <td>0.010977</td>\n",
       "      <td>1.491629e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.080375</td>\n",
       "      <td>0.035170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cncldist</th>\n",
       "      <td>0.054976</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>8.892704e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.058298</td>\n",
       "      <td>0.051654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>st_senate</th>\n",
       "      <td>0.047948</td>\n",
       "      <td>0.009301</td>\n",
       "      <td>1.617140e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.067099</td>\n",
       "      <td>0.028797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guards</th>\n",
       "      <td>0.047141</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>1.956440e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.058157</td>\n",
       "      <td>0.036126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip_city</th>\n",
       "      <td>0.043368</td>\n",
       "      <td>0.015992</td>\n",
       "      <td>1.867341e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.076297</td>\n",
       "      <td>0.010440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>steward</th>\n",
       "      <td>0.033697</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>4.223010e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.043260</td>\n",
       "      <td>0.024134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sidewalk</th>\n",
       "      <td>0.027387</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>3.012631e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.051110</td>\n",
       "      <td>0.003664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_type</th>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>3.334963e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.039793</td>\n",
       "      <td>0.013935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>borocode</th>\n",
       "      <td>0.015584</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>1.212371e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.035913</td>\n",
       "      <td>-0.004746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curb_loc</th>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>9.381388e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>-0.012594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.006113</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>5.124614e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.019451</td>\n",
       "      <td>-0.007226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            importance    stddev       p_value  n  p99_high   p99_low\n",
       "tree_dbh      0.103443  0.011060  1.544459e-05  5  0.126215  0.080671\n",
       "boro_ct       0.088029  0.009525  1.619399e-05  5  0.107641  0.068417\n",
       "spc_common    0.087082  0.014203  8.199706e-05  5  0.116327  0.057837\n",
       "nta           0.067274  0.004654  2.732187e-06  5  0.076857  0.057690\n",
       "st_assem      0.064464  0.004840  3.784412e-06  5  0.074430  0.054499\n",
       "month         0.062583  0.009903  7.278448e-05  5  0.082974  0.042193\n",
       "problems      0.061570  0.008490  4.229973e-05  5  0.079050  0.044090\n",
       "cb_num        0.060440  0.009493  7.069079e-05  5  0.079987  0.040894\n",
       "spc_genus     0.057772  0.010977  1.491629e-04  5  0.080375  0.035170\n",
       "cncldist      0.054976  0.001613  8.892704e-08  5  0.058298  0.051654\n",
       "st_senate     0.047948  0.009301  1.617140e-04  5  0.067099  0.028797\n",
       "guards        0.047141  0.005350  1.956440e-05  5  0.058157  0.036126\n",
       "zip_city      0.043368  0.015992  1.867341e-03  5  0.076297  0.010440\n",
       "steward       0.033697  0.004644  4.223010e-05  5  0.043260  0.024134\n",
       "sidewalk      0.027387  0.011522  3.012631e-03  5  0.051110  0.003664\n",
       "user_type     0.026864  0.006279  3.334963e-04  5  0.039793  0.013935\n",
       "borocode      0.015584  0.009874  1.212371e-02  5  0.035913 -0.004746\n",
       "curb_loc      0.006627  0.009335  9.381388e-02  5  0.025847 -0.012594\n",
       "year          0.006113  0.006478  5.124614e-02  5  0.019451 -0.007226"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = predictor.feature_importance(train_df)\n",
    "feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SettingWithCopyError",
     "evalue": "\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSettingWithCopyError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m train_clean \u001b[38;5;241m=\u001b[39m all_df\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;28mlen\u001b[39m(train_clean)]\n\u001b[0;32m     17\u001b[0m test_clean \u001b[38;5;241m=\u001b[39m all_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mlen\u001b[39m(train_clean):]\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtest_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhealth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m test_clean\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5260\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5264\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5265\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4552\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 4552\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4660\u001b[0m, in \u001b[0;36mNDFrame._update_inplace\u001b[1;34m(self, result, verify_is_copy)\u001b[0m\n\u001b[0;32m   4658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   4659\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m-> 4660\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_update_cacher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverify_is_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_is_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3821\u001b[0m, in \u001b[0;36mNDFrame._maybe_update_cacher\u001b[1;34m(self, clear, verify_is_copy, inplace)\u001b[0m\n\u001b[0;32m   3818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_is_copy:\n\u001b[1;32m-> 3821\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_setitem_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreferent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clear:\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4247\u001b[0m, in \u001b[0;36mNDFrame._check_setitem_copy\u001b[1;34m(self, t, force)\u001b[0m\n\u001b[0;32m   4236\u001b[0m     t \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA value is trying to be set on a copy of a slice from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindexing.html#returning-a-view-versus-a-copy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4244\u001b[0m     )\n\u001b[0;32m   4246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 4247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SettingWithCopyError(t)\n\u001b[0;32m   4248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4249\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(t, SettingWithCopyWarning, stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level())\n",
      "\u001b[1;31mSettingWithCopyError\u001b[0m: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"
     ]
    }
   ],
   "source": [
    "# 種目ごとの平均直径とその差を特徴量にする\n",
    "test_clean['health'] = 0\n",
    "all_df = pd.concat([train_clean, test_clean], axis=0)   \n",
    "tmp_df = all_df[['tree_dbh', 'spc_common', ]]\n",
    "dbh_mean_common = (pd.DataFrame(tmp_df.groupby('spc_common').mean()['tree_dbh']))\n",
    "dbh_mean_common.rename(columns={'tree_dbh': 'dbh_mean_common'}, inplace=True)\n",
    "all_df = pd.merge(all_df, dbh_mean_common, on='spc_common', how='left')\n",
    "all_df['dbh_diff_common'] = all_df['tree_dbh'] - all_df['dbh_mean_common']\n",
    "'''\n",
    "tmp_df = all_df[['tree_dbh', 'spc_genus', ]]\n",
    "dbh_mean_genus = (pd.DataFrame(tmp_df.groupby('spc_genus').mean()['tree_dbh']))\n",
    "dbh_mean_genus.rename(columns={'tree_dbh': 'dbh_mean_genus'}, inplace=True)\n",
    "all_df = pd.merge(all_df, dbh_mean_genus, on='spc_genus', how='left')\n",
    "all_df['dbh_diff_genus'] = all_df['tree_dbh'] - all_df['dbh_mean_genus']\n",
    "'''\n",
    "train_clean = all_df.iloc[:len(train_clean)]\n",
    "test_clean = all_df.iloc[len(train_clean):]\n",
    "test_clean.drop('health', axis=1, inplace=True)\n",
    "test_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# trainとtestに含まれるproblemは全く同じ\\n# problemのonehotカラムを用意\\nproblem_list = ['BranchLights', 'BranchOther', 'MetalGrates', 'RootOther', 'Stones', 'Sneakers', 'TrunkLights',\\n                 'TrunkOther', 'WiresRope', 'NoProblem']\\t\\n\\nfor problem in problem_list:\\n    train_clean.loc[:, problem] = 0\\n    test_clean.loc[:, problem] = 0\\n\\ntrain_clean['problem_count'] = 0\\ntest_clean['problem_count'] = 0\\n\\n# problemlistにあったらonehotする\\nfor i in train_clean.index:\\n    p_count = 0\\n    for problem in problem_list:\\n        if(problem in train_clean.loc[i, 'problems']):\\n            train_clean.loc[i, problem] = 1\\n            if(problem != 'NoProblem'):\\n                p_count+=1\\n    train_clean.loc[i, 'problem_count'] = p_count\\n\\nfor i in test_clean.index:\\n    p_count = 0\\n    for problem in problem_list:\\n        if(problem in test_clean.loc[i, 'problems']):\\n            test_clean.loc[i, problem] = 1\\n            if(problem != 'NoProblem'):\\n                p_count+=1\\n    test_clean.loc[i, 'problem_count'] = p_count\\n\\ntest_clean.head()\\n\""
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# trainとtestに含まれるproblemは全く同じ\n",
    "# problemのonehotカラムを用意\n",
    "problem_list = ['BranchLights', 'BranchOther', 'MetalGrates', 'RootOther', 'Stones', 'Sneakers', 'TrunkLights',\n",
    "                 'TrunkOther', 'WiresRope', 'NoProblem']\t\n",
    "\n",
    "for problem in problem_list:\n",
    "    train_clean.loc[:, problem] = 0\n",
    "    test_clean.loc[:, problem] = 0\n",
    "\n",
    "train_clean['problem_count'] = 0\n",
    "test_clean['problem_count'] = 0\n",
    "\n",
    "# problemlistにあったらonehotする\n",
    "for i in train_clean.index:\n",
    "    p_count = 0\n",
    "    for problem in problem_list:\n",
    "        if(problem in train_clean.loc[i, 'problems']):\n",
    "            train_clean.loc[i, problem] = 1\n",
    "            if(problem != 'NoProblem'):\n",
    "                p_count+=1\n",
    "    train_clean.loc[i, 'problem_count'] = p_count\n",
    "\n",
    "for i in test_clean.index:\n",
    "    p_count = 0\n",
    "    for problem in problem_list:\n",
    "        if(problem in test_clean.loc[i, 'problems']):\n",
    "            test_clean.loc[i, problem] = 1\n",
    "            if(problem != 'NoProblem'):\n",
    "                p_count+=1\n",
    "    test_clean.loc[i, 'problem_count'] = p_count\n",
    "\n",
    "test_clean.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## やりたい実験\n",
    "- あらゆるエンコーディング方法の比較 ok\n",
    "- 地区ごとの予測モデルの実装 \n",
    "- 365日で一周するsin, cos 意味無し\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tree_dbh', 'curb_loc', 'health', 'steward', 'guards', 'sidewalk',\n",
       "       'user_type', 'problems', 'spc_common', 'nta', 'nta_name', 'borocode',\n",
       "       'boro_ct', 'boroname', 'zip_city', 'cb_num', 'st_senate', 'st_assem',\n",
       "       'cncldist', 'year', 'month', 'spc_genus', 'dbh_mean_common',\n",
       "       'dbh_diff_common'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error! categorical is not same\n",
      "足りないカラム: {'cb_num'}\n"
     ]
    }
   ],
   "source": [
    "# エンコーディングパート\n",
    "\n",
    "# すべてのカテゴリカル変数\n",
    "all_categorical_cols = ['curb_loc', 'steward', 'guards', 'sidewalk',\n",
    "       'user_type', 'problems', 'spc_common', 'nta', 'nta_name', 'borocode',\n",
    "       'boro_ct', 'boroname', 'zip_city', 'cb_num', 'st_senate', 'st_assem',\n",
    "       'cncldist', 'year', 'month',  'spc_genus', ]\n",
    "\n",
    "# \n",
    "\n",
    "# 落とすカラム\n",
    "drop_cols = ['nta_name', 'boroname', 'boro_ct',\n",
    "              'nta', 'borocode', 'zip_city', 'st_senate',\n",
    "              'st_assem', 'cncldist', 'dbh_mean_common', 'tree_dbh',\n",
    "              ]#'',\n",
    "# カウントエンコーディング\n",
    "ce_columns = ['curb_loc',  'steward', 'guards', 'sidewalk',\n",
    "       'user_type',  'spc_common', 'spc_genus', 'problems', \n",
    "       'year', 'month'\n",
    "]\n",
    "# 'nta',\n",
    "#  \n",
    "# \n",
    "\n",
    "# ターゲットエンコーディング\n",
    "te_columns = []\n",
    "# ラベルエンコーディング\n",
    "le_columns = ['cb_num']\n",
    "encoding_cals = list(drop_cols + ce_columns + te_columns + drop_cols)\n",
    "if(encoding_cals != all_categorical_cols):\n",
    "    print('error! categorical is not same')\n",
    "    print('足りないカラム:', set(all_categorical_cols)-set(encoding_cals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットエンコーディングパート\n",
    "if(len(te_columns) > 0):\n",
    "    # onehot正解ラベルの作成\n",
    "    for i in range(3):\n",
    "        train_clean['onehot' + str(i)] = 0\n",
    "        train_clean['onehot' + str(i)] = train_clean['onehot' + str(i)].mask(train_clean['health'] == i, 1)\n",
    "\n",
    "    # ターゲットエンコーディングのカラムを3つずつに分ける\n",
    "    te_columns_list = []\n",
    "\n",
    "    for te_column in te_columns:\n",
    "        tmp_list = []\n",
    "        for i in range(3):\n",
    "            train_clean[te_column + '-te' + str(i)] = train_clean[te_column]\n",
    "            test_clean[te_column + '-te' + str(i)] = test_clean[te_column]\n",
    "            tmp_list.append(te_column + '-te' + str(i))\n",
    "        te_columns_list.append(tmp_list)\n",
    "        train_clean.drop(te_column, axis=1, inplace=True)\n",
    "        test_clean.drop(te_column, axis=1, inplace=True)\n",
    "\n",
    "    te_columns_list = np.array(te_columns_list)\n",
    "\n",
    "    # ターゲットエンコーディング\n",
    "    for i in range(3):\n",
    "        te_target = te_columns_list[:, i]\n",
    "        target_enc = TargetEncoder(target_type='binary', random_state=42)\n",
    "        \n",
    "        train_clean[te_target] = target_enc.fit_transform(train_clean[te_target], train_clean['onehot' + str(i)])\n",
    "        # テストデータにtransform\n",
    "        test_clean[te_target]  = target_enc.transform(test_clean[te_target])\n",
    "\n",
    "    # onehot正解ラベルを削除\n",
    "    for i in range(3):\n",
    "        dropcol = 'onehot' + str(i)\n",
    "        train_clean.drop(dropcol, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# 目的変数を指定\n",
    "y = train_clean['health']\n",
    "\n",
    "# 使わないcolumnを削除\n",
    "train_clean_drop = train_clean.drop(['health'], axis=1)\n",
    "train_clean_drop = train_clean_drop.drop(drop_cols, axis=1)\n",
    "test_clean_drop = test_clean.drop(drop_cols,axis=1)\n",
    "\n",
    "# ラベルエンコーディングパート\n",
    "le = LabelEncoder()\n",
    "for column in le_columns:\n",
    "    train_clean_drop[column] = le.fit_transform(train_clean_drop[column])\n",
    "    test_clean_drop[column] = le.transform(test_clean_drop[column])\n",
    "\n",
    "# カテゴリカル変数を指定\n",
    "for col in le_columns:\n",
    "    train_clean_drop[col] = train_clean_drop[col].astype('category')\n",
    "    test_clean_drop[col] = test_clean_drop[col].astype('category')  \n",
    "\n",
    "# カウントエンコーディングパート\n",
    "# mergeしてfit, そのあとtransform\n",
    "all_df = pd.concat([train_clean_drop, test_clean_drop], axis=0)\n",
    "\n",
    "# count encoding\n",
    "cencoder = ce.CountEncoder(cols=ce_columns)\n",
    "cencoder.fit(all_df)\n",
    "\n",
    "# count encodingを適用\n",
    "train_clean_drop = cencoder.transform(train_clean_drop)\n",
    "test_clean_drop = cencoder.transform(test_clean_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>curb_loc</th>\n",
       "      <th>steward</th>\n",
       "      <th>guards</th>\n",
       "      <th>sidewalk</th>\n",
       "      <th>user_type</th>\n",
       "      <th>problems</th>\n",
       "      <th>spc_common</th>\n",
       "      <th>cb_num</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>spc_genus</th>\n",
       "      <th>dbh_diff_common</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37270</td>\n",
       "      <td>29409</td>\n",
       "      <td>29510</td>\n",
       "      <td>18509</td>\n",
       "      <td>22298</td>\n",
       "      <td>24288</td>\n",
       "      <td>2579</td>\n",
       "      <td>52</td>\n",
       "      <td>30331</td>\n",
       "      <td>3921</td>\n",
       "      <td>7853</td>\n",
       "      <td>1.653354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37270</td>\n",
       "      <td>2068</td>\n",
       "      <td>7366</td>\n",
       "      <td>21177</td>\n",
       "      <td>22298</td>\n",
       "      <td>24288</td>\n",
       "      <td>254</td>\n",
       "      <td>18</td>\n",
       "      <td>9355</td>\n",
       "      <td>6054</td>\n",
       "      <td>3613</td>\n",
       "      <td>-5.157480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37270</td>\n",
       "      <td>29409</td>\n",
       "      <td>29510</td>\n",
       "      <td>21177</td>\n",
       "      <td>22298</td>\n",
       "      <td>1529</td>\n",
       "      <td>2579</td>\n",
       "      <td>58</td>\n",
       "      <td>30331</td>\n",
       "      <td>6054</td>\n",
       "      <td>7853</td>\n",
       "      <td>13.653354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37270</td>\n",
       "      <td>29409</td>\n",
       "      <td>29510</td>\n",
       "      <td>18509</td>\n",
       "      <td>6031</td>\n",
       "      <td>24288</td>\n",
       "      <td>2104</td>\n",
       "      <td>58</td>\n",
       "      <td>9355</td>\n",
       "      <td>1113</td>\n",
       "      <td>2104</td>\n",
       "      <td>3.515684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37270</td>\n",
       "      <td>29409</td>\n",
       "      <td>29510</td>\n",
       "      <td>21177</td>\n",
       "      <td>22298</td>\n",
       "      <td>4455</td>\n",
       "      <td>4339</td>\n",
       "      <td>9</td>\n",
       "      <td>9355</td>\n",
       "      <td>3921</td>\n",
       "      <td>4339</td>\n",
       "      <td>3.567181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   curb_loc  steward  guards  sidewalk  user_type  problems  spc_common  \\\n",
       "0     37270    29409   29510     18509      22298     24288        2579   \n",
       "1     37270     2068    7366     21177      22298     24288         254   \n",
       "2     37270    29409   29510     21177      22298      1529        2579   \n",
       "3     37270    29409   29510     18509       6031     24288        2104   \n",
       "4     37270    29409   29510     21177      22298      4455        4339   \n",
       "\n",
       "  cb_num   year  month  spc_genus  dbh_diff_common  \n",
       "0     52  30331   3921       7853         1.653354  \n",
       "1     18   9355   6054       3613        -5.157480  \n",
       "2     58  30331   6054       7853        13.653354  \n",
       "3     58   9355   1113       2104         3.515684  \n",
       "4      9   9355   3921       4339         3.567181  "
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0 start\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 546\n",
      "[LightGBM] [Info] Number of data points in the train set: 15987, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[518]\ttraining's multi_logloss: 0.225401\tvalid_1's multi_logloss: 0.795617\n",
      "fold0 f1_score: 0.3464\n",
      "fold1 start\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 545\n",
      "[LightGBM] [Info] Number of data points in the train set: 15987, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[588]\ttraining's multi_logloss: 0.205717\tvalid_1's multi_logloss: 0.784046\n",
      "fold1 f1_score: 0.3574\n",
      "fold2 start\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 544\n",
      "[LightGBM] [Info] Number of data points in the train set: 15987, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[529]\ttraining's multi_logloss: 0.224522\tvalid_1's multi_logloss: 0.794667\n",
      "fold2 f1_score: 0.3495\n",
      "fold3 start\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 546\n",
      "[LightGBM] [Info] Number of data points in the train set: 15987, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[473]\ttraining's multi_logloss: 0.245006\tvalid_1's multi_logloss: 0.811447\n",
      "fold3 f1_score: 0.3489\n",
      "fold4 start\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 546\n",
      "[LightGBM] [Info] Number of data points in the train set: 15988, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koba_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[514]\ttraining's multi_logloss: 0.236332\tvalid_1's multi_logloss: 0.764604\n",
      "fold4 f1_score: 0.3596\n",
      "CV: 0.3524\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "valid_scores = []\n",
    "models = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_clean_drop, y)):\n",
    "    print(f'fold{fold} start')\n",
    "    X_train, y_train = train_clean_drop.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_valid, y_valid = train_clean_drop.iloc[valid_idx], y.iloc[valid_idx]\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train, weight=compute_sample_weight(class_weight='balanced', y=y_train).astype('float32'))\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, weight=np.ones(len(X_valid)).astype('float32'))\n",
    "\n",
    "    '''\n",
    "    params = {'objective': 'multiclass',\n",
    " 'metric': 'multi_logloss',\n",
    " 'num_class': 3,\n",
    " 'seed': 0,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 0.0015923361719036968,\n",
    " 'lambda_l2': 0.004786876640032096,\n",
    " 'num_leaves': 252,\n",
    " 'feature_fraction': 0.8,\n",
    " 'bagging_fraction': 0.9931160872840541,\n",
    " 'bagging_freq': 7,\n",
    " 'min_child_samples': 5,\n",
    " 'num_iterations': 1000}\n",
    " '''\n",
    "    verbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\n",
    "    params = {'objective': 'multiclass',\n",
    " 'metric': 'multi_logloss',\n",
    " 'num_class': 3,\n",
    " 'seed': 0,\n",
    " 'num_iterations': 1000}\n",
    "    \n",
    "    model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      valid_sets=[lgb_train, lgb_eval],\n",
    "                      num_boost_round=1000,\n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=50, \n",
    "                                verbose=True), # early_stopping用コールバック関数\n",
    "                           lgb.log_evaluation(verbose_eval)], # コマンドライン出力用コールバック関数))\n",
    "                        # feval=f1,\n",
    "                        # categorical_feature=le_columns,\n",
    "                    )\n",
    "\n",
    "    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "    score = f1_score(y_valid, y_pred_max, average='macro')\n",
    "    print(f'fold{fold} f1_score: {score:.4f}')\n",
    "    valid_scores.append(score)\n",
    "    models.append(model)\n",
    "print(f'CV: {np.mean(valid_scores):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlgb_train = lgb.Dataset(train_clean_drop, y, weight=compute_sample_weight(class_weight='balanced', y=y).astype('float32'))\\nverbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\\nparams = {'objective': 'multiclass',\\n'metric': 'multi_logloss',\\n'num_class': 3,\\n'seed': 0\\n}\\n\\nmodel = lgb.train(params,\\n                    lgb_train,\\n                    num_boost_round=520,\\n                )\\n\""
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全データを利用して学習\n",
    "'''\n",
    "lgb_train = lgb.Dataset(train_clean_drop, y, weight=compute_sample_weight(class_weight='balanced', y=y).astype('float32'))\n",
    "verbose_eval = -1 # この数字を1にすると学習時のスコア推移がコマンドライン表示される\n",
    "params = {'objective': 'multiclass',\n",
    "'metric': 'multi_logloss',\n",
    "'num_class': 3,\n",
    "'seed': 0\n",
    "}\n",
    "\n",
    "model = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=520,\n",
    "                )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_clean_drop)\n",
    "y_pred_max = np.argmax(y_pred, axis=1) # 最尤と判断したクラスの値にする\n",
    "sample_df[1] = y_pred_max\n",
    "# sample_df.to_csv('C:/python/signate/data/BEST2-nta-commondiff-dbhDrop.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dbh_diff_common</td>\n",
       "      <td>13296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spc_common</td>\n",
       "      <td>5746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>month</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cb_num</td>\n",
       "      <td>5249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>problems</td>\n",
       "      <td>4663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spc_genus</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_type</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sidewalk</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>guards</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>steward</td>\n",
       "      <td>1166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>year</td>\n",
       "      <td>1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>curb_loc</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature  Importance\n",
       "11  dbh_diff_common       13296\n",
       "6        spc_common        5746\n",
       "9             month        5641\n",
       "7            cb_num        5249\n",
       "5          problems        4663\n",
       "10        spc_genus        4096\n",
       "4         user_type        2208\n",
       "3          sidewalk        1380\n",
       "2            guards        1296\n",
       "1           steward        1166\n",
       "8              year        1059\n",
       "0          curb_loc         460"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importance = model.feature_importance()\n",
    "feature_names = model.feature_name()\n",
    "\n",
    "df_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "# importtane順に並び替え\n",
    "df_importance = df_importance.sort_values('Importance', ascending=False)\n",
    "df_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
